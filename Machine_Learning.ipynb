{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progetto di Machine Learning\n",
    "\n",
    "Analisi del Dataset **Dry Bean** (disponibile presso il sito [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/dataset/602/dry+bean+dataset)).\n",
    "\n",
    "Il Dataset **Dry Bean** è stato costruito a partire da immagini ad alta risoluzione di semi di fagioli secchi appartenenti a 7 specie differenti; grazie a queste immagini è stato possibile estrarre 16 caratteristiche sui semi.\n",
    "\n",
    "Nella cella sottostante si procede al download del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tramite la libreria ufficiale di UCI si importa la funzione \"fetch_ucirepo()\" \n",
    "# utile per il download del dataset.\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# Download del dataset tramite il codice identificativo (602)\n",
    "dry_bean = fetch_ucirepo(id=602) \n",
    "  \n",
    "# Si suddivide il dataset in features (X) e labels (y)\n",
    "X = dry_bean.data.features \n",
    "y = dry_bean.data.targets \n",
    "y = y.to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizzazione dei dati\n",
    "\n",
    "Dopo aver suddivido il dataset in *features* (**X**) e *labels* (**y**) si procede con la visualizzazione dei dati contenuti all'interno del dataset **Dry Bean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informazioni generali sul Dataset\n",
    "print(f\"\\nNumero di campioni -> {X.shape[0]}\")\n",
    "print(f\"\\nNumero di features -> {X.shape[1]}\")\n",
    "\n",
    "print(\"\\nUlteriori informazioni sul dataset:\")\n",
    "    \n",
    "# Si escludono le colonne contenenti informazioni necessarie o vuote\n",
    "dry_bean.variables.drop(columns=[\"description\", \"demographic\", \"units\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si noti che all'interno del Dataset sono presenti **13611 campioni**, ognuno con **16 features** (esclusa la classe di appartenenza).\n",
    "\n",
    "Nessuna features presenta **valori mancanti**, sono presenti solo **valori interi** (*Area* e *ConvexArea*) e **valori decimali** (tutte le restanti features).\n",
    "\n",
    "\n",
    "All'interno del Dataset **Dry Bean** scaricato tramite la libreria *ucimlrepo* è presente una descrizione per ogni features.\n",
    "\n",
    "Verrà mostrata a seguire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDescrizione delle features del dataset (Inclusa la classe):\")\n",
    "\n",
    "for i in range(len(dry_bean.variables)):\n",
    "    print(f\"{dry_bean.variables['name'][i]:<15} -> {dry_bean.variables['description'][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisi dei dati \n",
    "\n",
    "Conteggio delle occorrenze per ogni classe e successiva visualizzazione grafica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Si stampa l'intestazione per il conteggio di campioni per classe\n",
    "print(f\"\\nNumero di campioni per classe:\")\n",
    "\n",
    "# Si ottengono i valori univoci (beans) e le loro frequenze (counts) dal vettore delle etichette y\n",
    "# np.unique restituisce i valori unici e, con return_counts=True, anche le loro occorrenze\n",
    "beans, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "# Si ordinano i beans in base al numero di occorrenze (dal più piccolo al più grande)\n",
    "sort = np.argsort(counts)  # Si ottengono gli indici degli elementi ordinati\n",
    "beans = beans[sort]        # Si riordina l'array dei beans usando questi indici\n",
    "counts = counts[sort]      # Si riordina l'array dei conteggi usando gli stessi indici\n",
    "\n",
    "# Si stampa il numero di occorrenze per ogni classe di fagioli\n",
    "# Il formato {bean:<10} allinea a sinistra con una larghezza di 10 caratteri\n",
    "for bean, count in zip(beans, counts):\n",
    "   print(f\"{bean:<10} -> {count} occorrenze\")\n",
    "\n",
    "# Si crea una figura con dimensioni specificate\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Si crea un grafico a barre dove:\n",
    "# - l'asse x rappresenta le diverse classi di fagioli (beans)\n",
    "# - l'asse y rappresenta il numero di occorrenze (counts)\n",
    "# - le barre sono colorate di grigio con bordo nero\n",
    "plt.bar(beans, counts, color=\"gray\", edgecolor='black')\n",
    "\n",
    "# Si aggiungono etichette testuali sopra ogni barra con il valore esatto delle occorrenze\n",
    "# count + 50 posiziona il testo 50 unità sopra la barra\n",
    "# ha=\"center\" allinea orizzontalmente il testo al centro della barra\n",
    "for bean, count in zip(beans, counts):\n",
    "   plt.text(bean, count + 50, f\"{count}\", ha=\"center\") \n",
    "   \n",
    "# Si aggiungono etichette agli assi e un titolo al grafico\n",
    "plt.xlabel(\"Beans\")  # Etichetta per l'asse x\n",
    "plt.ylabel(\"Occorrenze\")  # Etichetta per l'asse y\n",
    "plt.title(\"Distribuzione delle occorrenze per classe\", fontsize=16)  # Titolo del grafico\n",
    "\n",
    "# Si ruotano le etichette sull'asse x di 45 gradi per migliorare la leggibilità\n",
    "plt.xticks(beans, rotation=45)  \n",
    "\n",
    "# Si aggiunge una griglia orizzontale (solo per l'asse y) con linee tratteggiate\n",
    "plt.grid(axis=\"y\", linestyle=\"--\")  \n",
    "\n",
    "# Si visualizza il grafico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si può notare come siano presenti sostanziali differenze nella quantità di occorrenze per ogni classe, la classe meno rappresentata **BOMBAY**, ad esempio, è altamente inferiore rispetto la classe maggiormente rappresentata **DERMASON**.\n",
    "\n",
    "Queste situazioni di *imbalance* e *over rapresentation* possono influire negativamente sui modelli, questo perchè nel caso di *imbalance* il modello potrebbe non imparare a riconoscere correttamente la classe, mentre nella situazione opposta di *over rapresentation* il modello potrebbe imparare troppo dettagliatamente a riconoscere correttamente la classe, portando ad un **OVERFITTING** o ad un **Bias** nei suoi confronti.\n",
    "\n",
    "Tra le possibili soluzioni da considerare esistono l'**OVERSAMPLING** (ovvero la generazione sintetica di dati della classe minoritaria per bilanciare la differenza) e l'**UNDERSAMPLING** (ovvero la rimozione di dati dalla classe maggioritaria per bilanciare la differenza).\n",
    "\n",
    "### Boxplot\n",
    "\n",
    "Si prosegue con la visualizzazione dei dati alla ricerca di valori anomali (*outliers*) tramite **BOXPLOT**.\n",
    "\n",
    "Per ogni Classe *BARBUNYA, BOMBAY, CALI, DERMASON, HOROZ, SEKER, SIRA* viene analizzato ogni attributo presente nel dataset **X** (features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Si crea la directory per i boxplot se non esiste\n",
    "dir = \"1 - BOXPLOT\"\n",
    "path_dir = os.path.join(os.getcwd(), dir)\n",
    "\n",
    "if not os.path.exists(path_dir):\n",
    "    os.makedirs(path_dir)\n",
    "\n",
    "# Si crea la matrice per tenere traccia degli outlier: righe=feature, colonne=classi\n",
    "# outliers[i][j] contiene il numero di outlier per la feature i nella classe j\n",
    "outliers = np.zeros((len(X.columns), len(beans) + 1), dtype=int)\n",
    "\n",
    "# Si dividono le feature in due gruppi per creare due PNG per ogni classe\n",
    "features_group1 = X.columns[:8]  # Prime 8 feature\n",
    "features_group2 = X.columns[8:]  # Restanti 8 feature\n",
    "\n",
    "# Si itera per ogni classe di fagioli j (con l'aggiunta di un ciclo per l'aggiunta dei boxplot generali)\n",
    "for j in range(len(beans) + 1):\n",
    "    \n",
    "    # GRUPPO 1 - Prime 8 feature: si crea una figura con 2 righe e 4 colonne di subplot\n",
    "    fig1, axs1 = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    \n",
    "    # Si inserisce un titolo a seconda della situazione\n",
    "    if j == len(beans):\n",
    "        fig1.suptitle(f\"Boxplot - General 1\", fontsize=25)\n",
    "    else:\n",
    "        fig1.suptitle(f\"Boxplot - Class: {beans[j]} 1\", fontsize=25)\n",
    "    axs1 = axs1.flatten()  # Si trasforma la matrice 2x4 in un array 1D per facilità d'uso\n",
    "    \n",
    "    # GRUPPO 2 - Restanti 8 feature: si crea una seconda figura con 2 righe e 4 colonne\n",
    "    fig2, axs2 = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    \n",
    "    # Si inserisce un titolo a seconda della situazione\n",
    "    if j == len(beans):\n",
    "        fig2.suptitle(f\"Boxplot - General 2\", fontsize=25)\n",
    "    else:\n",
    "        fig2.suptitle(f\"Boxplot - Class: {beans[j]} 2\", fontsize=25)\n",
    "    axs2 = axs2.flatten()  # Si trasforma la matrice 2x4 in un array 1D\n",
    "    \n",
    "    # Si itera sulle prime 8 feature (GRUPPO 1)\n",
    "    for i, feature in enumerate(features_group1):\n",
    "        ax = axs1[i]  # Si seleziona il subplot corrente\n",
    "        \n",
    "        if j == len(beans):\n",
    "            \n",
    "            # Si selezionano i dati dell'intero dataset (Boxplot - General)\n",
    "            datas = X[feature]\n",
    "        else:\n",
    "            \n",
    "            # Si filtrano solo i dati della classe corrente (j) per la feature corrente (i)\n",
    "            datas = X.iloc[(y == beans[j]), X.columns.get_loc(feature)]\n",
    "        \n",
    "        # Si crea il boxplot con media e mediana evidenziate\n",
    "        ax.boxplot(datas, \n",
    "                   showmeans=True,  # Si mostra la media\n",
    "                   meanline=True,   # Si visualizza la media come linea\n",
    "                   flierprops=dict(marker=\"x\", markeredgecolor=\"purple\"),  # Formattazione outlier\n",
    "                   meanprops=dict(color=\"red\", linestyle=\"--\"),            # Formattazione linea media\n",
    "                   medianprops=dict(color=\"blue\", linestyle=\"--\"))         # Formattazione linea mediana\n",
    "        \n",
    "        ax.set_title(f\"Feature: {feature}\")  # Titolo del subplot\n",
    "        \n",
    "        # Si calcolano gli outlier usando l'approccio del range interquartile (IQR)\n",
    "        Q1 = datas.quantile(0.25)  # Primo quartile\n",
    "        Q3 = datas.quantile(0.75)  # Terzo quartile\n",
    "        IQR = Q3 - Q1              # Range interquartile\n",
    "        \n",
    "        # Si identificano gli outlier come valori fuori da [Q1-1.5*IQR, Q3+1.5*IQR]\n",
    "        low_outliers = datas[datas < Q1 - 1.5 * IQR]           # Outlier inferiori\n",
    "        high_outliers = datas[datas > Q3 + 1.5 * IQR]          # Outlier superiori\n",
    "        tot_outliers = len(low_outliers) + len(high_outliers)  # Totale outlier\n",
    "        \n",
    "        # Si memorizza il numero di outlier nella matrice\n",
    "        feature_idx = X.columns.get_loc(feature)\n",
    "        if j == len(beans):\n",
    "            \n",
    "            # Si memorizzano gli outlier generali nell'ultima colonna\n",
    "            outliers[feature_idx][-1] = tot_outliers\n",
    "        else:\n",
    "            \n",
    "            # Si memorizzano gli outlier per classe\n",
    "            outliers[feature_idx][j] = tot_outliers\n",
    "    \n",
    "    # Si itera sulle restanti 8 feature (GRUPPO 2) - procedimento identico al precedente\n",
    "    for i, feature in enumerate(features_group2):\n",
    "        ax = axs2[i]\n",
    "        \n",
    "        if j == len(beans):\n",
    "            datas = X[feature]\n",
    "        else:\n",
    "            datas = X.iloc[(y == beans[j]), X.columns.get_loc(feature)]\n",
    "        \n",
    "        ax.boxplot(datas, \n",
    "                   showmeans=True, \n",
    "                   meanline=True,\n",
    "                  flierprops=dict(marker=\"x\", markeredgecolor=\"purple\"),\n",
    "                  meanprops=dict(color=\"red\", linestyle=\"--\"),\n",
    "                  medianprops=dict(color=\"blue\", linestyle=\"--\")) \n",
    "       \n",
    "        ax.set_title(f\"Feature: {feature}\")\n",
    "       \n",
    "        Q1 = datas.quantile(0.25)\n",
    "        Q3 = datas.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        low_outliers = datas[datas < Q1 - 1.5 * IQR]\n",
    "        high_outliers = datas[datas > Q3 + 1.5 * IQR]\n",
    "        tot_outliers = len(low_outliers) + len(high_outliers)\n",
    "\n",
    "        feature_idx = X.columns.get_loc(feature)\n",
    "        if j == len(beans):\n",
    "            outliers[feature_idx][-1] = tot_outliers\n",
    "        else:\n",
    "            outliers[feature_idx][j] = tot_outliers\n",
    "   \n",
    "    # Si aggiunge legenda al primo grafico (GRUPPO 1) \n",
    "    handles1 = [\n",
    "        plt.Line2D([0], [0], color=\"red\", linestyle=\"--\", label=\"Media\"),\n",
    "        plt.Line2D([0], [0], color=\"blue\", linestyle=\"--\", label=\"Mediana\"),\n",
    "        plt.Line2D([0], [0], marker=\"x\", color=\"purple\", linestyle=\"\", label=\"Outliers (x)\", markersize=10)\n",
    "    ]\n",
    "    fig1.legend(handles=handles1, loc='upper right', ncol=3, fontsize=15)\n",
    "    \n",
    "    # Si aggiunge legenda al secondo grafico (GRUPPO 2)\n",
    "    handles2 = [\n",
    "        plt.Line2D([0], [0], color=\"red\", linestyle=\"--\", label=\"Media\"),\n",
    "        plt.Line2D([0], [0], color=\"blue\", linestyle=\"--\", label=\"Mediana\"),\n",
    "        plt.Line2D([0], [0], marker=\"x\", color=\"purple\", linestyle=\"\", label=\"Outliers (x)\", markersize=10)\n",
    "    ]\n",
    "    fig2.legend(handles=handles2, loc=\"upper right\", ncol=3, fontsize=15)\n",
    "    \n",
    "    # Si salvano i grafici in file separati\n",
    "    plt.figure(fig1.number)\n",
    "    if j == len(beans):\n",
    "        plt.savefig(f\"{dir}/ALL_features_1_8\")\n",
    "    else:\n",
    "        plt.savefig(f\"{dir}/{beans[j]}_features_1_8\")\n",
    "    \n",
    "    plt.figure(fig2.number)\n",
    "    if j == len(beans):\n",
    "        plt.savefig(f\"{dir}/ALL_features_9_16\")\n",
    "    else:\n",
    "        plt.savefig(f\"{dir}/{beans[j]}_features_9_16\")\n",
    "    \n",
    "    # Si chiudono le figure per liberare memoria\n",
    "    plt.close(fig1)\n",
    "    plt.close(fig2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spiegazione Boxplot**\n",
    "\n",
    "La base inferiore della scatola rappresenta il **primo quartile** ($Q1$), ovvero il valore che separa il 25% inferiore dei dati; mentre la base superiore rappresenta il **terzo quartile** ($Q3$), ovvero il valore che separa il 75% superiore dei dati.\n",
    "\n",
    "La lunghezza della scatola rappresenta l'**intervallo interquartile** ($IQR$), ovvero la differenza tra il **terzo quartile** ($Q3$) e il **primo quartile** ($Q1$), al suo interno sono presenti la metà centrale dei dati. \n",
    "\n",
    "Inoltre sono visibili la linea **mediana** (nei file .png rappresentanti i Boxplot è visualizzata come una linea di colore blu) che rappresenta il **secondo interquartile** ($Q2$) e la linea della **media** (nei file .png rappresentanti i Boxplot è visualizzata come una linea di colore rosso) che rappresenta il valore medio di tutti i dati.\n",
    "\n",
    "I baffi, invece, sono linee che si estendono dall'estremità della scatola sino ai dati che non sono considerati outliers.\n",
    "\n",
    "Gli **outliers**, infine, sono dati (*valori anomali*) che si trovano al di fuori dei baffi e non rispettano le formule $Q1 - 1.5 * IQR$ e $Q3 + 1.5 * IQR$(le quali determinano quali sono i valori anomali).\n",
    "\n",
    "**Valutazione dei Boxplot ottenuti**\n",
    "\n",
    "Visualizzando i vari file .png si può notare come le distribuzioni dei dati presentano quasi sempre outliers, di seguito si verifica il totale di outliers ottenuti rispetto ogni features, e la sua percentuale corrispondente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"Numero di Outliers per ogni classe e feature:\")\n",
    "\n",
    "# Si crea un DataFrame con le colonne per ogni classe, più una colonna \"General\"\n",
    "columns_names = list(beans) + [\"General\"]\n",
    "outliers_df = pd.DataFrame(\n",
    "    data=outliers, \n",
    "    index=X.columns,       \n",
    "    columns=columns_names\n",
    ")\n",
    "\n",
    "# Si sommano gli outlier escludendo la colonna General\n",
    "outliers_df[\"TOT (escluso General)\"] = outliers_df.iloc[:, :-1].sum(axis=1)  \n",
    "\n",
    "# Stampa del DataFrame\n",
    "print(outliers_df.to_string())\n",
    "\n",
    "print(\"\\nPercentuale corrispondente:\")\n",
    "beans_counts = dict(zip(beans, counts))\n",
    "\n",
    "# Si aggiunge il totale dei campioni per il calcolo della percentuale generale\n",
    "beans_counts_with_general = beans_counts.copy()\n",
    "beans_counts_with_general[\"General\"] = sum(counts)\n",
    "\n",
    "# Si calcolano le percentuali\n",
    "outliers_df_perc = outliers_df.iloc[:, :-1].div(pd.Series(beans_counts_with_general), axis=1) * 100\n",
    "\n",
    "totale_campioni = sum(counts)\n",
    "outliers_df_perc[\"TOT (escluso General)\"] = (outliers_df[\"TOT (escluso General)\"] / totale_campioni * 100).round(2)\n",
    "\n",
    "outliers_df_perc = outliers_df_perc.round(2)\n",
    "\n",
    "# Stampa del DataFrame\n",
    "print(outliers_df_perc.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si può verificare che per ciacuna classe, nessuna feature presenta una percentuale maggiore del 10% di **outliers**, mentre sul totale non viene superato il 5% (escludendo la colonna General). Può rendersi comunque necessaria la gestione dei valori anomali. \n",
    "\n",
    "### Istogramma\n",
    "\n",
    "Per sicurezza si verifica la distribuzione dei dati in un istogramma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si crea la directory per gli istogrammi se non esiste\n",
    "dir = \"2 - HISTOGRAM\"\n",
    "path_dir = os.path.join(os.getcwd(), dir)\n",
    "\n",
    "if not os.path.exists(path_dir):\n",
    "   os.makedirs(path_dir)\n",
    "   \n",
    "# Si dividono le feature in due gruppi per creare due PNG per ogni classe\n",
    "features_group1 = X.columns[:8]  # Prime 8 feature\n",
    "features_group2 = X.columns[8:]  # Restanti 8 feature\n",
    "\n",
    "# Si itera per ogni classe di fagioli j (con l'aggiunta di un ciclo per l'aggiunta degli istogrammi generali)\n",
    "for j in range(len(beans) + 1):\n",
    "   \n",
    "   # GRUPPO 1 - Prime 8 feature: si crea una figura con 2 righe e 4 colonne di subplot\n",
    "   fig1, axs1 = plt.subplots(2, 4, figsize=(20, 10))\n",
    "   \n",
    "   # Si inserisce un titolo a seconda della situazione\n",
    "   if j == len(beans):\n",
    "      fig1.suptitle(f\"Histogram - General 1\", fontsize=25)\n",
    "   else:\n",
    "      fig1.suptitle(f\"Histogram - Class: {beans[j]} 1\", fontsize=25)\n",
    "   axs1 = axs1.flatten()  # Si trasforma la matrice 2x4 in un array 1D per facilità d'uso\n",
    "   \n",
    "   # GRUPPO 2 - Restanti 8 feature: si crea una seconda figura con 2 righe e 4 colonne\n",
    "   fig2, axs2 = plt.subplots(2, 4, figsize=(20, 10))\n",
    "   \n",
    "   # Si inserisce un titolo a seconda della situazione\n",
    "   if j == len(beans):\n",
    "      fig2.suptitle(f\"Histogram - General 2\", fontsize=25)\n",
    "   else:\n",
    "      fig2.suptitle(f\"Histogram - Class: {beans[j]} 2\", fontsize=25)\n",
    "   axs2 = axs2.flatten()  # Si trasforma la matrice 2x4 in un array 1D\n",
    "\n",
    "   # Si itera sulle prime 8 feature (GRUPPO 1)\n",
    "   for i, feature in enumerate(features_group1):\n",
    "      ax = axs1[i]  # Si seleziona il subplot corrente\n",
    "      \n",
    "      if j == len(beans):\n",
    "         \n",
    "         # Si selezionano i dati dell'intero dataset\n",
    "         datas = X[feature]\n",
    "      else:\n",
    "         \n",
    "         # Si filtrano solo i dati della classe corrente (j) per la feature corrente (i)\n",
    "         datas = X.iloc[(y == beans[j]), X.columns.get_loc(feature)]\n",
    "       \n",
    "      # Si calcolano media e deviazione standard\n",
    "      mean = np.mean(datas)\n",
    "      std = np.std(datas)\n",
    "       \n",
    "      # Si crea l'istogramma con media e deviazione standard evidenziate\n",
    "      ax.hist(datas, bins=\"auto\", edgecolor=\"black\", color=\"purple\", alpha=0.5)\n",
    "      ax.axvline(mean, color=\"red\", linestyle=\"--\")  # Si visualizza la media\n",
    "      ax.axvline(mean - std, color=\"blue\", linestyle=\"dashed\")  # Si visualizza media - deviazione standard\n",
    "      ax.axvline(mean + std, color=\"blue\", linestyle=\"dashed\")  # Si visualizza media + deviazione standard\n",
    "      \n",
    "      # Si imposta il titolo del subplot con i valori statistici\n",
    "      if j == len(beans):\n",
    "         ax.set_title(f\"Feature: {feature}\\nμ = {mean:.2f} - σ = {std:.2f}\") \n",
    "      else:\n",
    "         ax.set_title(f\"Feature: {feature}\\nClass {beans[j]}: μ = {mean:.2f} - σ = {std:.2f}\") \n",
    "\n",
    "   # Si itera sulle restanti 8 feature (GRUPPO 2) - procedimento identico al precedente\n",
    "   for i, feature in enumerate(features_group2):\n",
    "      ax = axs2[i] \n",
    "      \n",
    "      if j == len(beans):\n",
    "         datas = X[feature]\n",
    "      else:\n",
    "         datas = X.iloc[(y == beans[j]), X.columns.get_loc(feature)]\n",
    "      \n",
    "      mean = np.mean(datas)\n",
    "      std = np.std(datas)\n",
    "      \n",
    "      ax.hist(datas, bins=\"auto\", edgecolor=\"black\", color=\"purple\", alpha=0.5)\n",
    "      ax.axvline(mean, color=\"red\", linestyle=\"--\")  \n",
    "      ax.axvline(mean - std, color=\"blue\", linestyle=\"dashed\")  \n",
    "      ax.axvline(mean + std, color=\"blue\", linestyle=\"dashed\")  \n",
    "      \n",
    "      if j == len(beans):\n",
    "         ax.set_title(f\"Feature: {feature}\\nμ = {mean:.2f} - σ = {std:.2f}\") \n",
    "      else:\n",
    "         ax.set_title(f\"Feature: {feature}\\nClass {beans[j]}: μ = {mean:.2f} - σ = {std:.2f}\") \n",
    "   \n",
    "   # Si aggiunge legenda al primo grafico (GRUPPO 1) \n",
    "   handles1 = [\n",
    "      plt.Line2D([0], [0], color=\"red\", linestyle=\"--\", label=\"Media (μ)\"),\n",
    "      plt.Line2D([0], [0], color=\"blue\", linestyle=\"--\", label=\"Deviazione Standard (σ)\"),\n",
    "   ]\n",
    "   fig1.legend(handles=handles1, loc='upper right', ncol=2, fontsize=15)\n",
    "\n",
    "   # Si aggiunge legenda al secondo grafico (GRUPPO 2)\n",
    "   handles2 = [\n",
    "      plt.Line2D([0], [0], color=\"red\", linestyle=\"--\", label=\"Media (μ)\"),\n",
    "      plt.Line2D([0], [0], color=\"blue\", linestyle=\"--\", label=\"Deviazione Standard (σ)\"),\n",
    "   ]\n",
    "   fig2.legend(handles=handles2, loc='upper right', ncol=2, fontsize=15)\n",
    "   \n",
    "   # Si salvano i grafici in file separati\n",
    "   plt.figure(fig1.number)\n",
    "   if j == len(beans):\n",
    "      plt.savefig(f\"{dir}/ALL_features_1_8\")\n",
    "   else:\n",
    "      plt.savefig(f\"{dir}/{beans[j]}_features_1_8\")\n",
    "   \n",
    "   plt.figure(fig2.number)\n",
    "   if j == len(beans):\n",
    "      plt.savefig(f\"{dir}/ALL_features_9_16\")\n",
    "   else:\n",
    "      plt.savefig(f\"{dir}/{beans[j]}_features_9_16\")\n",
    "   \n",
    "   # Si chiudono le figure per liberare memoria\n",
    "   plt.close(fig1)\n",
    "   plt.close(fig2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spiegazione Istogramma**\n",
    "\n",
    "L'istogramma è una rappresentazione grafica della *distribuzione di frequenza dei dati* che permette di visualizzarne la forma complessiva. Ogni barra rappresenta la frequenza dei valori all'interno di un determinato intervallo.\n",
    "\n",
    "Negli istogrammi generati, la linea verticale rossa rappresenta la **media** (μ) della distribuzione, mentre le linee blu tratteggiate indicano la **deviazione standard** (σ), mostrando l'intervallo μ ± σ. Questo intervallo, in una distribuzione normale, contiene circa il 68% dei dati.\n",
    "\n",
    "L'istogramma fornisce informazioni importanti come la *tendenza centrale dei dati*, la *forma della distribuzione*, la *dispersione dei dati* e la *presenza di valori anomali* o di raggruppamenti inaspettati\n",
    "\n",
    "Attraverso l'analisi degli istogrammi per ciascuna classe e feature, si possono identificare caratteristiche distintive delle diverse varietà di fagioli e comprendere la variabilità all'interno di ciascuna classe.\n",
    "\n",
    "**Valutazione degli Istogrammi ottenuti**\n",
    "\n",
    "In linea generale, la maggior parte delle distribuzioni segue una forma a campana, suggerendo una **distribuzione normale** (o simil normale), alcune classi presentano una distribuzione più ampia (maggiore deviazione standard).\n",
    "Le distribuzioni Generali invece presentano distribuzioni con più picchi, evidenziando una differenza nelle distribuzioni delle features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Si crea la directory per le matrici di correlazione se non esiste\n",
    "dir = \"3 - MATRIX CORRELATION\"\n",
    "path_dir = os.path.join(os.getcwd(), dir)\n",
    "\n",
    "if not os.path.exists(path_dir):\n",
    "    os.makedirs(path_dir)\n",
    "\n",
    "# Si itera per ogni classe di fagioli j\n",
    "for j in range(len(beans)):\n",
    "    \n",
    "    # Si filtrano solo i dati della classe corrente (j)\n",
    "    datas = X.iloc[(y == beans[j])]\n",
    "    \n",
    "    # Si calcola la matrice di correlazione per la classe corrente\n",
    "    correlation_matrix = datas.corr()\n",
    "    \n",
    "    # Si crea la figura per la matrice di correlazione\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Si visualizza la matrice come heatmap con annotazioni\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "    \n",
    "    # Si aggiunge il titolo alla figura\n",
    "    plt.title(f\"Feature Correlation Matrix - Class {beans[j]}\", fontsize=25)\n",
    "    \n",
    "    # Si salva la figura nella directory appropriata\n",
    "    plt.savefig(f\"{dir}/correlation_matrix_{beans[j]}.png\")\n",
    "    \n",
    "    # Si chiude la figura per liberare memoria\n",
    "    plt.close()\n",
    "\n",
    "# Si calcola la matrice di correlazione generale (considerando tutti i dati)\n",
    "correlation_matrix_all = X.corr()\n",
    "\n",
    "# Si crea la figura per la matrice di correlazione generale\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Si visualizza la matrice generale come heatmap con annotazioni\n",
    "sns.heatmap(correlation_matrix_all, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "# Si aggiunge il titolo alla figura generale\n",
    "plt.title(\"General Correlation Matrix\", fontsize=25)\n",
    "\n",
    "# Si salva la figura generale nella directory appropriata\n",
    "plt.savefig(f\"{dir}/ALL_correlation_matrix.png\")\n",
    "\n",
    "# Si chiude la figura per liberare memoria\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spiegazione della Matrice di Correlazione**\n",
    "\n",
    "La matrice di correlazione è una tabella che mostra i coefficienti di correlazione tra coppie di variabili (feature). Ogni cella della matrice rappresenta il coefficiente di correlazione di Pearson tra due feature, indicando la forza e la direzione della relazione lineare tra esse.\n",
    "\n",
    "Il coefficiente di correlazione varia da -1.0 a +1.0, dove:\n",
    "- Un valore di +1.0 indica una **correlazione positiva perfetta** (quando una variabile aumenta, l'altra aumenta proporzionalmente)\n",
    "- Un valore di -1.0 indica una **correlazione negativa perfetta** (quando una variabile aumenta, l'altra diminuisce proporzionalmente)\n",
    "- Un valore di 0 indica l'**assenza di correlazione lineare**\n",
    "\n",
    "Nella visualizzazione, i colori caldi (rossi) indicano **correlazioni positive**, mentre i colori freddi (blu) indicano **correlazioni negative**. L'intensità del colore rappresenta la forza della correlazione: colori più intensi indicano correlazioni più forti.\n",
    "\n",
    "La diagonale della matrice mostra la correlazione di ciascuna variabile con se stessa, che è sempre 1.0. La matrice è simmetrica rispetto alla diagonale, poiché la correlazione tra la variabile A e B è la stessa che tra B e A.\n",
    "\n",
    "Analizzando le matrici di correlazione per le diverse classi di fagioli, si possono identificare:\n",
    "- Feature fortemente correlate che potrebbero essere ridondanti\n",
    "- Relazioni specifiche tra caratteristiche morfologiche per ciascuna varietà\n",
    "- Differenze nei pattern di correlazione tra le diverse classi, che potrebbero riflettere differenze biologiche fondamentali\n",
    "\n",
    "La matrice di correlazione è quindi uno strumento essenziale per comprendere le interrelazioni tra le caratteristiche.\n",
    "\n",
    "**Valutazione delle Matrici di Correlazione ottenute**\n",
    "\n",
    "Verificando i risultati ottenuti, si può affermare che le varie matrici di coorelazione ottenute dalle diverse classi di fagioli hanno struttura simile, sia nelle correlazioni positive che negative. Ciò si può notare anche dall'analisi della matrice di correlazione generale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "ll data pre-processing, o pre-elaborazione dei dati, è una fase fondamentale nel processo di preparazione dei dati per l'applicazione di algoritmi di machine learning. Questa fase coinvolge una serie di azioni volte a pulire, trasformare e preparare i dati in modo da renderli adatti all'analisi e all'addestramento degli algoritmi di machine learning.\n",
    "\n",
    "### Standardizzazione\n",
    "\n",
    "Tecnica comune utilizzata per garantire che le varie feature abbiano la stessa scala. \n",
    "\n",
    "A seguito della standardizzazione si ottiene una **distribuzione delle feature** con *media* = 0 e *deviazione standard* = 1, denominata distribuzione normale standardizzata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Funzione per la standardizzazione del dataset\n",
    "# Si inizializza lo StandardScaler per standardizzare i dati\n",
    "\n",
    "scaler = StandardScaler() # Si dichiara fuori dalla funzione così da poterlo riusare \n",
    "def standardization(X_train, X_test):\n",
    "\n",
    "    # Si applica la standardizzazione ai dati : trasforma i dati in modo che \n",
    "    # abbiano media = 0 e deviazione standard = 1\n",
    "    X_train_std = scaler.fit_transform(X_train)\n",
    "    X_test_std = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_std, X_test_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizzazione\n",
    "\n",
    "Tecnica comune utilizzata per scalare le feature in un intervallo specifico, tipicamente [0,1]. \n",
    "\n",
    "La normalizzazione mappa i valori minimi e massimi di ciascuna feature rispettivamente a 0 e 1, mantenendo la distribuzione proporzionale dei dati originali. Questo approccio è particolarmente utile quando le feature hanno range diversi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Si inizializza il MinMaxScaler per normalizzare i dati \n",
    "normalizer = MinMaxScaler() # Si dichiara fuori dalla funzione così da poterlo riusare\n",
    "\n",
    "def normalization(X_train, X_test):\n",
    "    \n",
    "    # Si applica la normalizzazione ai dati: trasforma i dati in modo che\n",
    "    # siano nell'intervallo [0,1] mantenendo le proporzioni originali\n",
    "    X_train_norm = normalizer.fit_transform(X_train)\n",
    "    X_test_norm = normalizer.transform(X_test)\n",
    "    \n",
    "    return X_train_norm, X_test_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregazione delle feature\n",
    "\n",
    "L'aggregazione di feature rappresenta una tecnica avanzata di **riduzione della dimensionalità** che opera attraverso il raggruppamento gerarchico di feature simili. Questo approccio permette di combinare variabili correlate in modo interpretabile, preservando la struttura informativa del dataset.\n",
    "\n",
    "Basandoci sulla General Correlation Matrix ottenuta, si può affermare che è presente un'elevata multicollineareità tra features. La matrice evidenzia chiaramente gruppi di variabili fortemente correlate (con coefficienti superiori a 0.9), di conseguenza l'aggregazione può essere un'ottima soluzione per ridurre la dimensionalità senza perdere informazioni significative.\n",
    "\n",
    "Per procedere correttamente con l'aggregazione **è necessario standardizzare e/o normalizzare i dati**. Questo passaggio preliminare è fondamentale per garantire che il processo di clustering non sia influenzato dalle diverse scale delle variabili originali ma si basi esclusivamente sui pattern di correlazione.\n",
    "\n",
    "Il dendrogramma risultante dall'analisi gerarchica fornisce una rappresentazione visiva delle relazioni tra le feature e permette di identificare il numero ottimale di cluster da utilizzare per l'aggregazione finale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "\n",
    "# Si crea la directory per i dendrogrammi se non esiste\n",
    "dir = \"4 - DENDROGRAM\"\n",
    "path_dir = os.path.join(os.getcwd(), dir)\n",
    "\n",
    "if not os.path.exists(path_dir):\n",
    "    os.makedirs(path_dir)\n",
    "\n",
    "# Si crea una figura con due subplot affiancati orizzontalmente\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# PRIMO SUBPLOT - Dendrogramma con dati standardizzati\n",
    "# Si calcola la matrice di distanza tra feature standardizzate \n",
    "# Nota: si traspone la matrice per ottenere feature × osservazioni\n",
    "distance_matrix_std = hierarchy.distance.pdist(scaler.fit_transform(X).T)\n",
    "\n",
    "# Si applica il linkage gerarchico con metodo Ward\n",
    "linkage_matrix_std = hierarchy.linkage(distance_matrix_std, method=\"ward\")\n",
    "\n",
    "# Si crea il dendrogramma per i dati standardizzati nel primo subplot\n",
    "dendrogram_std = hierarchy.dendrogram(\n",
    "    linkage_matrix_std,\n",
    "    labels=X.columns,\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=10,\n",
    "    color_threshold=0,\n",
    "    ax=ax1  # Si specifica il primo subplot\n",
    ")\n",
    "\n",
    "# Si impostano titolo e assi per il primo subplot\n",
    "ax1.set_title(\"Dendrogram (StandardScaler)\", fontsize=15)\n",
    "ax1.set_xlabel(\"Feature\")\n",
    "ax1.set_ylabel(\"Dissimilarity\")\n",
    "\n",
    "# SECONDO SUBPLOT - Dendrogramma con dati normalizzati\n",
    "# Si calcola la matrice di distanza tra feature normalizzate\n",
    "distance_matrix_norm = hierarchy.distance.pdist(normalizer.fit_transform(X).T)\n",
    "\n",
    "# Si applica il linkage gerarchico con metodo Ward\n",
    "linkage_matrix_norm = hierarchy.linkage(distance_matrix_norm, method=\"ward\")\n",
    "\n",
    "# Si crea il dendrogramma per i dati normalizzati nel secondo subplot\n",
    "dendrogram_norm = hierarchy.dendrogram(\n",
    "    linkage_matrix_norm,\n",
    "    labels=X.columns,\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=10,\n",
    "    color_threshold=0,\n",
    "    ax=ax2  # Si specifica il secondo subplot\n",
    ")\n",
    "\n",
    "# Si impostano titolo e assi per il secondo subplot\n",
    "ax2.set_title(\"Dendrogram (MinMaxScaler)\", fontsize=15)\n",
    "ax2.set_xlabel(\"Feature\")\n",
    "ax2.set_ylabel(\"Dissimilariy\")\n",
    "\n",
    "# Si aggiunge un titolo generale alla figura\n",
    "fig.suptitle(\"Dendrogram with standardization - Dendrogram with normalization\", fontsize=20)\n",
    "\n",
    "# Si salva la figura combinata nella directory appropriata\n",
    "plt.savefig(f\"{dir}/Dendrogram.png\")\n",
    "\n",
    "# Si chiude la figura per liberare memoria\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dal file PNG ottenuto, si può verificare come a seconda del metodo di pre-processing utilizzato variano i risultati dell'aggregazione.\n",
    "\n",
    "Si può affermare che nel caso dell'aggregazione successiva alla standardizzazione si individuano **6 cluster** differenti.\n",
    "\n",
    "Nel caso dell'aggregazione successiva alla normalizzazione, si individuano **4 cluster** principali.\n",
    "\n",
    "Si aggiunge alla cartella DENDROGRAM la visualizzazione della separazione per cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "\n",
    "# Si rimane nella directory precedentemente creata\n",
    "dir = \"4 - DENDROGRAM\"\n",
    "path_dir = os.path.join(os.getcwd(), dir)\n",
    "\n",
    "# Si crea una figura con due subplot affiancati orizzontalmente\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# PRIMO SUBPLOT - Dendrogramma con dati standardizzati\n",
    "\n",
    "# Si determina il numero di cluster e il threshold \n",
    "# threshold è il parametro che determina quali cluster evidenziare\n",
    "n_clusters_std = 6\n",
    "threshold_std = linkage_matrix_std[-(n_clusters_std-1), 2]\n",
    "\n",
    "# Si crea il dendrogramma per i dati standardizzati nel primo subplot\n",
    "dendrogram_std = hierarchy.dendrogram(\n",
    "    linkage_matrix_std,\n",
    "    labels=X.columns,\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=10,\n",
    "    color_threshold=threshold_std,\n",
    "    ax=ax1  # Si specifica il primo subplot\n",
    ")\n",
    "\n",
    "# Si impostano titolo e assi per il primo subplot\n",
    "ax1.set_title(\"Dendrogram (StandardScaler)\", fontsize=15)\n",
    "ax1.set_xlabel(\"Feature\")\n",
    "ax1.set_ylabel(\"Dissimilarity\")\n",
    "\n",
    "# Si determina il numero di cluster e il threshold \n",
    "# threshold è il parametro che determina quali cluster evidenziare\n",
    "n_clusters_norm = 4\n",
    "threshold_norm = linkage_matrix_norm[-(n_clusters_norm-1), 2]\n",
    "\n",
    "# Si crea il dendrogramma per i dati normalizzati nel secondo subplot\n",
    "dendrogram_norm = hierarchy.dendrogram(\n",
    "    linkage_matrix_norm,\n",
    "    labels=X.columns,\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=10,\n",
    "    color_threshold=threshold_norm,\n",
    "    ax=ax2  # Si specifica il secondo subplot\n",
    ")\n",
    "\n",
    "# Si impostano titolo e assi per il secondo subplot\n",
    "ax2.set_title(\"Dendrogram (MinMaxScaler)\", fontsize=15)\n",
    "ax2.set_xlabel(\"Feature\")\n",
    "ax2.set_ylabel(\"Dissimilariy\")\n",
    "\n",
    "# Si aggiunge un titolo generale alla figura\n",
    "fig.suptitle(\"Dendrogram with standardization - Dendrogram with normalization\", fontsize=20)\n",
    "\n",
    "# Si salva la figura combinata nella directory appropriata\n",
    "plt.savefig(f\"{dir}/Dendrogram_colored.png\")\n",
    "\n",
    "# Si chiude la figura per liberare memoria\n",
    "plt.close()\n",
    "# Si stampa un messaggio informativo per i cluster ottenuti dai dati standardizzati\n",
    "print(\"Cluster ottenuti dai dati aggregati con standardizzazione\")\n",
    "\n",
    "# Si ottengono le etichette dei cluster dal linkage matrix dei dati standardizzati\n",
    "# utilizzando il numero di cluster specificato\n",
    "cluster_labels = hierarchy.fcluster(linkage_matrix_std, n_clusters_std, criterion='maxclust')\n",
    "\n",
    "# Si estraggono i valori unici delle etichette dei cluster\n",
    "unique_clusters = np.unique(cluster_labels)\n",
    "\n",
    "# Si iterano i cluster unici\n",
    "for i in unique_clusters:\n",
    "    \n",
    "   # Si estraggono i nomi delle feature appartenenti al cluster corrente\n",
    "   cluster_members = [col for col, label in zip(X.columns, cluster_labels) if label == i]\n",
    "   \n",
    "   # Si stampano le informazioni sul cluster\n",
    "   print(f\"Cluster {i}: {cluster_members}\")\n",
    "\n",
    "# Si stampa un messaggio informativo per i cluster ottenuti dai dati normalizzati\n",
    "print(\"\\nCluster ottenuti dai dati aggregati con normalizzazione\")\n",
    "\n",
    "# Si ottengono le etichette dei cluster dal linkage matrix dei dati normalizzati\n",
    "# utilizzando il numero di cluster specificato\n",
    "cluster_labels = hierarchy.fcluster(linkage_matrix_norm, n_clusters_norm, criterion='maxclust')\n",
    "\n",
    "# Si estraggono i valori unici delle etichette dei cluster\n",
    "unique_clusters = np.unique(cluster_labels)\n",
    "\n",
    "# Si iterano i cluster unici\n",
    "for i in unique_clusters:\n",
    "    \n",
    "   # Si estraggono i nomi delle feature appartenenti al cluster corrente\n",
    "   cluster_members = [col for col, label in zip(X.columns, cluster_labels) if label == i]\n",
    "   \n",
    "   # Si stampano le informazioni sul cluster\n",
    "   print(f\"Cluster {i}: {cluster_members}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una volta creati i cluster (confrontabili con il dendrogramma visualizzato precedentemente), si può procedere all'aggregazione dei dati standardizzati o normalizzati con l'apposita funzione (*i cluster otttenuti possono variare leggermente a causa della diversa funzione utilizzata*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import FeatureAgglomeration\n",
    "from typing import Literal\n",
    "\n",
    "def agglomeration(X_train, X_test, type: Literal[\"normalization\", \"standardization\"]):\n",
    "    \n",
    "    # Si verifica il parametro\n",
    "    if type not in [\"normalization\", \"standardization\"]:\n",
    "        raise ValueError(\"cluster_type deve essere 'normalization' o 'standardization'\")\n",
    "    \n",
    "    # Si imposta il numero corretto di cluster da generare a seconda della \n",
    "    # tecnica precentemente utilizzata\n",
    "    if type == \"normalization\":\n",
    "        clusters = n_clusters_norm\n",
    "    elif type == \"standardization\":\n",
    "        clusters = n_clusters_std\n",
    "    \n",
    "    # Si utilizza FeatureAgglomerator per ridurre la dimensionalità \n",
    "    # raggruppando feature simili basandosi su criteri di distanza.\n",
    "    # Si utilizzano le stesse impostazioni utilizzate per il dendrogramma\n",
    "    # così da ottenere i medesimi cluster\n",
    "    agglomerator = FeatureAgglomeration(\n",
    "        n_clusters=clusters,\n",
    "        metric=\"euclidean\",\n",
    "        linkage=\"ward\"\n",
    "    )\n",
    "    \n",
    "    # Si applica l'aggregazione ai dati: Si aggregano le feature sulla base\n",
    "    # delle impostazioni di FeatureAgglomeration\n",
    "    X_train_agg = agglomerator.fit_transform(X_train)\n",
    "    X_test_agg = agglomerator.transform(X_test)\n",
    "    \n",
    "    return X_train_agg, X_test_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling\n",
    "\n",
    "L'undersampling è una tecnica di riduzione dei dati che consiste nella selezione di un sottoinsieme rappresentativo di osservazioni da un dataset originale più grande. Questa strategia viene utilizzata principalmente per ridurre il costo computazionale e migliorare l'efficienza dell'analisi.\n",
    "\n",
    "L'undersampling offre numerosi vantaggi in termini di efficienza, tuttavia è importante considerare il rischio di perdita di informazioni.\n",
    "\n",
    "Si usa la tecnica ClusterCentroids, individuando i centroidi rappresentativi delle classi maggioritarie (tramite Kmeans) e riducendole a un numero di istanze pari a quello della classe minoritaria. Nonostante ClusterCentroids generi dati sintetici preserva meglio la distribuzione originale dei dati, cattura la struttura interna di ogni classe maggioritaria e riduce il rumore mantenendo l'informazione essenziale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "# Si inizializza ClusterCentroids \n",
    "cc = ClusterCentroids(random_state=21) # Si dichiara fuori dalla funzione così da poterlo riusare\n",
    "\n",
    "def undersampling(X_train, y_train):\n",
    "    \n",
    "    # Si applica l'undersampling ai dati di train\n",
    "    X_train_und, y_train_und = cc.fit_resample(X_train, y_train)\n",
    "    \n",
    "    return X_train_und, y_train_und\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling\n",
    "\n",
    "L'oversampling è una tecnica di espansione dei dati che consiste nell'aumentare artificialmente il numero di istanze nelle classi minoritarie di un dataset sbilanciato. Questa strategia viene utilizzata principalmente per bilanciare la distribuzione delle classi e migliorare le prestazioni dei modelli di classificazione.\n",
    "\n",
    "L'oversampling offre numerosi vantaggi in termini di bilanciamento del dataset, tuttavia è importante considerare il rischio di overfitting sui dati sintetici generati.\n",
    "\n",
    "Si usa la tecnica SMOTE (Synthetic Minority Over-sampling Technique), che genera esempi sintetici per le classi minoritarie creando nuove istanze lungo i segmenti che collegano i k vicini più prossimi di ciascun esempio. A differenza della duplicazione casuale, SMOTE crea esempi che seguono la distribuzione originale dei dati, introducendo variabilità che aiuta il modello a generalizzare meglio e a definire confini decisionali più robusti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Si inizializza Smote\n",
    "smote = SMOTE(random_state=21) # Si dichiara fuori dalla funzione così da poterlo riusare\n",
    "\n",
    "def oversampling(X_train, y_train):\n",
    "    \n",
    "    # Si applica l'oversampling sui dati di train\n",
    "    X_train_und, y_train_und = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    return X_train_und, y_train_und"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di seguito si genera una lista di liste contenente le varie combinazioni possibili data dalle combinazioni delle varie tecniche di preprocessing utilizzate.\n",
    "\n",
    "Successivamente si definisce una funzione per la combianzione di quest'ultime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Si definisce la lista di liste techniques contenente tutte le possibili combinazioni\n",
    "# delle varie tecniche di pre processing (compreso il caso in cui non si utilizzano tecniche)\n",
    "techniques = [\n",
    "    [],\n",
    "    [\"standardization\"],\n",
    "    [\"normalization\"],\n",
    "    [\"undersampling\"],\n",
    "    [\"oversampling\"],\n",
    "    [\"standardization\", \"undersampling\"],\n",
    "    [\"standardization\", \"oversampling\"],\n",
    "    [\"normalization\", \"undersampling\"],\n",
    "    [\"normalization\", \"oversampling\"],\n",
    "    [\"standardization\", \"agglomeration\"],\n",
    "    [\"normalization\", \"agglomeration\"],\n",
    "    [\"standardization\", \"agglomeration\", \"undersampling\"],\n",
    "    [\"standardization\", \"agglomeration\", \"oversampling\"],\n",
    "    [\"normalization\", \"agglomeration\", \"undersampling\"],\n",
    "    [\"normalization\", \"agglomeration\", \"oversampling\"]\n",
    "]\n",
    "\n",
    "# Si definisce una funzione per la combinazione delle varie tecniche\n",
    "def combine_preprocessing(X, y, techniques):\n",
    "    \n",
    "    # Si utilizza lo split con stratificazione per mantenere il bilanciamento delle classe\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)\n",
    "\n",
    "    # Standardizzazione o normalizzazione (mutuamente esclusivi)\n",
    "    if \"standardization\" in techniques:\n",
    "        X_train, X_test = standardization(X_train, X_test)\n",
    "    elif \"normalization\" in techniques:\n",
    "        X_train, X_test = normalization(X_train, X_test)\n",
    "    \n",
    "    # Agglomerazione (DOPO standardizzazione/normalizzazione)\n",
    "    if \"agglomeration\" in techniques:\n",
    "        if \"standardization\" in techniques:\n",
    "            X_train, X_test = agglomeration(X_train, X_test, \"standardization\")\n",
    "        elif \"normalization\" in techniques:\n",
    "            X_train, X_test = agglomeration(X_train, X_test, \"normalization\")\n",
    "    \n",
    "    # Undersampling o oversampling (mutuamente esclusivi)\n",
    "    if \"undersampling\" in techniques:\n",
    "        X_train, y_train = undersampling(X_train, y_train)\n",
    "    elif \"oversampling\" in techniques:\n",
    "        X_train, y_train = oversampling(X_train, y_train)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelli di Classificazione\n",
    "\n",
    "I modelli di classificazione sono **algoritmi di machine learning** che assegnano un'etichetta di classe predefinita a nuove osservazioni, basandosi su un *training set* di esempi già etichettati. Questi modelli apprendono i pattern nei dati per effettuare previsioni su nuovi esempi. L'implementazione segue generalmente quattro fasi: **preparazione dei dati**, **selezione del modello appropriato** in base alle caratteristiche del problema, **addestramento sui dati di training** per ottimizzare i parametri interni e **valutazione delle performance** su dati non visti. È essenziale valutare le prestazioni con metriche appropriate come accuratezza, precisione e recall, e ottimizzare gli iperparametri.\n",
    "\n",
    "Si valuteranno una serie di algoritmi per verificare quale ha l'accuratezza maggiore e su quale dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si crea una lista per contenere i risultati e le caratteristiche dei migliori modelli\n",
    "best_configs_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "L'**albero decisionale** è un algoritmo di machine learning che prende decisioni seguendo una struttura ad albero, dove ogni nodo interno rappresenta un test su un attributo, ogni ramo corrisponde a un risultato del test e ogni foglia rappresenta una classe o una decisione finale. \n",
    "\n",
    "Gli alberi decisionali offrono notevoli vantaggi: sono facilmente interpretabili, gestiscono naturalmente dati categorici e numerici, trattano efficacemente valori mancanti e catturano relazioni non lineari senza richiedere trasformazioni preliminari dei dati. Hanno tuttavia la tendenza all'**overfitting**.\n",
    "\n",
    "La libreria *scikit-learn* mette a disposizione la classe [*DecisionTreeClassifier*](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) per implementare l'algoritmo di classificazione dell'albero decisionale. Quest'ultimo utilizza l'indice di Gini by default per misurare la qualità degli split. \n",
    "\n",
    "Si effettua un **tuning degli iperparametri** per definire il valore migliore per la **profondità** dell'albero (DecisionTrees) e quale criterio è migliore per la misurazione della **qualità degli split** (gini, entropy, log_loss), si eseguono una serie di passaggi:\n",
    "- Si definisce *StratifiedKFold*, un generatore di split utile per la preparazione di dati per la cross validation successiva. *StratifiedKFold* divide il dataset in 5 fold, durante la cross validation verranno eseguite 5 iterazioni, ogni iterazione userà 4 fold per il training e 1 fold per la validation.\n",
    "- Si utilizza *train_test_split* per splittare il dataset iniziale (**X**, **y**) in train e test con stratificazione (si utilizza solo il train per la cross validation) \n",
    "- Si utilizza la tecnica *GridSearchCV* come tecnica di cross validation in icerca la migliore combinazione di iperparametri provando automaticamente tutte le combinazioni specificate. Inoltre ha un costo computazionale inferiore rispetto le classiche tecniche di cross validation e restituisce il miglior modello.\n",
    "\n",
    "Per ogni combinazione di tecniche si restituisce l'accuratezza maggiore in corrispondenza della profondità ottimale dell'albero e del miglior criterio di separazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Si definisce skf, si divide il dataset in 5 fold con shuffle\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=21)\n",
    "\n",
    "# Si definisce una lista per contenere i risultati\n",
    "results_decisionTrees = []\n",
    "\n",
    "# Si definisce il classificatore DecisionTreeClassifier su cui effettuare\n",
    "# il tuning degli iperparametri\n",
    "clf = DecisionTreeClassifier(random_state=21)\n",
    "\n",
    "# Si definisce una lista di profondità dell'albero da testare \n",
    "# durante la cross validation\n",
    "max_depth = list(range(5,16)) # 5 - 15\n",
    "\n",
    "# Si definisce una lista dei possibili criteri da utilizzare\n",
    "# per misurare la qualità degli split\n",
    "criterion = ['gini', 'entropy']\n",
    "\n",
    "# Si divide il dataset iniziale in train e test con stratificazione \n",
    "# ovvero le occorrenze delle classi sono automaticamente bilanciate\n",
    "# tra train e test\n",
    "X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)\n",
    "\n",
    "\n",
    "# Si inizializzano le due variabili utilizzate successivamente per salvare il miglior\n",
    "# classificatore ttrovato tramite la grid search e cross validation\n",
    "best_decisionTree_score = -1\n",
    "best_decisionTree_clf = None\n",
    "\n",
    "# Si itera per tutte le combinazioni di tecniche di pre processing possibili \n",
    "# e precedentemente definite\n",
    "for technique in techniques:\n",
    "    \n",
    "    # Si definisce una lista per contenere i vari step per la cross validation\n",
    "    steps = []\n",
    "    \n",
    "    # Se la tecnica specificata tra virgolette è presente nella lista\n",
    "    # technique si aggiunge tra gli step della cross validation\n",
    "    if \"standardization\" in technique:\n",
    "        steps.append((\"scaler\", scaler))\n",
    "        \n",
    "        # Si salva il numero corretto di cluster per la standardizzazione \n",
    "        n_clusters = n_clusters_std\n",
    "        \n",
    "    elif \"normalization\" in technique:\n",
    "        steps.append((\"normalizer\", normalizer))\n",
    "        \n",
    "        # Si salva il numero corretto di cluster per la normalizzazione\n",
    "        n_clusters = n_clusters_std\n",
    "        \n",
    "    if \"agglomeration\" in technique:\n",
    "        \n",
    "        # Si definisce lo step dell'agglomerazione di feature con gli stessi \n",
    "        # parametri usati precedentemente \n",
    "        steps.append((\"agglomeration\", FeatureAgglomeration(\n",
    "            n_clusters=n_clusters,\n",
    "            metric=\"euclidean\",\n",
    "            linkage=\"ward\"\n",
    "        )))\n",
    "        \n",
    "    if \"undersampling\" in technique:\n",
    "        steps.append((\"undersampler\", cc))\n",
    "        \n",
    "    elif \"oversampling\" in technique:\n",
    "        steps.append((\"oversampler\", smote))\n",
    "    \n",
    "    # Si aggiunge il classificatore DecisionTreeClassifier come ultimo step\n",
    "    # della cross validation\n",
    "    steps.append((\"classifier\", clf))\n",
    "    \n",
    "    # Si crea la pipeline di step \n",
    "    # si utilizza ImbPipeline invece di Pipeline in quanto accetta \n",
    "    # le tecniche di sampling (oversampling/undersampling)\n",
    "    pipeline = ImbPipeline(steps)\n",
    "\n",
    "    # Si definisce il parametro di GridSearchCV per la ricerca della\n",
    "    # migliore profondità dell'albero e del miglior criterio\n",
    "    param_grid = {\n",
    "        \"classifier__max_depth\": max_depth,\n",
    "        \"classifier__criterion\": criterion\n",
    "    }\n",
    "    \n",
    "    # Si definisce GridSearchCV, la pipeline da seguire, i parametri,\n",
    "    # la tecnica per la cross validation (StratifiedKFold), lo scoring da \n",
    "    # utilizzare (balanced_accuracy in quanto il dataset è sbilanciato) e il\n",
    "    # numero di core da utilizzare contemporaneamente\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=skf,\n",
    "        scoring=\"balanced_accuracy\",\n",
    "        n_jobs=-1,                      # si utilizzano tutti i core disponibili\n",
    "        verbose=1                       # si mostrano degli aggiornamenti sull'andamento del codice\n",
    "    )\n",
    "\n",
    "    # Si esegue la cross validation sul train set, per ogni tecnica si \n",
    "    # verifica quale combinazione di profondità e criterio ottiene l'accuratezza maggiore\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Si verifica il risultato del modello, se migliore del precedente si salva il corrente\n",
    "    if grid_search.best_score_ > best_decisionTree_score:\n",
    "        best_decisionTree_score = grid_search.best_score_\n",
    "        best_decisionTree_clf = grid_search.best_estimator_\n",
    "    \n",
    "    # Si estrae solo il miglior risultato per questa tecnica\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    # Si definiscono i risultati per ogni tecnica, profondità e criterio\n",
    "    result = {\n",
    "        \"techniques_list\": technique,                                   # Si mantiene la lista di tecniche utilizzate per comodità\n",
    "        \"techniques\": ', '.join(technique),                             # Si converte la lista di tecniche in stringa per migliore visualizzazione\n",
    "        \"max_depth\": best_params[\"classifier__max_depth\"],              # Si salva la profondità\n",
    "        \"criterion\": best_params[\"classifier__criterion\"],              # Si salva il criterio\n",
    "        \"mean_test_score\": grid_search.best_score_,                     # Si salva il valore medio dell'accuratezza bilanciata ottenuto dalle 5 iterazioni della cross validation\n",
    "        \"std_test_score\": grid_search.cv_results_[\"std_test_score\"][    # Si salva il valore medio della deviazione standard (utile per verificare i risultati della cross validation)\n",
    "            grid_search.cv_results_[\"mean_test_score\"].argmax()\n",
    "        ]               \n",
    "    }\n",
    "        \n",
    "    # Si aggiunge il miglior risultato ai risultati finali\n",
    "    results_decisionTrees.append(result)\n",
    "    \n",
    "    # Si libera la memoria\n",
    "    del grid_search\n",
    "        \n",
    "# Si trasforma result in un pandas DataFrame\n",
    "results_decisionTrees = pd.DataFrame(results_decisionTrees)\n",
    "\n",
    "# Si restituisce il miglior risultato per ogni tecnica \n",
    "print(\"\\nMigliori configurazioni per ogni tecnica:\")\n",
    "print(results_decisionTrees[[\"techniques\", \"max_depth\", \"criterion\", \"mean_test_score\", \"std_test_score\"]].sort_values(\"mean_test_score\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Migliori configurazioni per ogni tecnica:\n",
    "                                   techniques  max_depth criterion  mean_test_score  std_test_score\n",
    "                  normalization, oversampling          8      gini         0.922296        0.006918\n",
    "                standardization, oversampling          9      gini         0.920509        0.003477\n",
    "                                 oversampling         10      gini         0.920508        0.005482\n",
    "                                                      10      gini         0.917425        0.006273\n",
    "                                normalization         10      gini         0.917425        0.006273\n",
    "                              standardization         10      gini         0.917425        0.006273\n",
    " standardization, agglomeration, oversampling         10      gini         0.904470        0.006552\n",
    "               standardization, agglomeration         10   entropy         0.900220        0.007012\n",
    "               standardization, undersampling          7   entropy         0.898660        0.003765\n",
    "                 normalization, undersampling          7   entropy         0.898262        0.005138\n",
    "                 normalization, agglomeration          8      gini         0.889432        0.001961\n",
    "   normalization, agglomeration, oversampling          9      gini         0.889157        0.002140\n",
    "standardization, agglomeration, undersampling          9      gini         0.884840        0.008821\n",
    "                                undersampling          6      gini         0.879965        0.015209\n",
    "  normalization, agglomeration, undersampling          7      gini         0.876228        0.005636\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_decisionTree_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si verificano i risultati del codice precedente:\n",
    "- Per ogni configurazione trovata, si applicano le medesime tecniche di preprocessing e si addestra un DecisionTree con gli ipeparametri ottimali.\n",
    "- Si esegue una valutazione delle performance sul test set.\n",
    "- Si visualizzano le matrici di confusione per ogni configurazione.\n",
    "- Si salvano i risultati della migliore configurazione. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (multilabel_confusion_matrix, \n",
    "                             ConfusionMatrixDisplay, \n",
    "                             balanced_accuracy_score, \n",
    "                             confusion_matrix\n",
    "                            )\n",
    "\n",
    "# Si crea la directory per i decision trees se non esiste\n",
    "dir = \"5 - DECISION TREES\"\n",
    "path_dir = os.path.join(os.getcwd(), dir)\n",
    "\n",
    "if not os.path.exists(path_dir):\n",
    "    os.makedirs(path_dir)\n",
    "\n",
    "# Si crea una lista per contenere i vari risultati\n",
    "result_decisionTrees = []\n",
    "\n",
    "# Si itera lungo le righe delle configurazioni migliori \n",
    "for row in results_decisionTrees.itertuples():\n",
    "    # Si salva la lista contenente le tecniche di pre processing \n",
    "    technique = row.techniques_list\n",
    "    \n",
    "    # Si divide il dataset iniziale in train e test con stratificazione\n",
    "    # random_state = 21 ci garantisce che per ogni iterazione otteniamo \n",
    "    # sempre le stesse separazioni\n",
    "    X_train, X_test, y_train, y_test = combine_preprocessing(X, y, technique)\n",
    "    \n",
    "    # Si salva la profondità associata   \n",
    "    max_depth = row.max_depth\n",
    "    \n",
    "    # Si salva il criterio associato\n",
    "    criterion = row.criterion\n",
    "    \n",
    "    # Si definisce DecisionTreeClassifier con gli iperparametri definiti\n",
    "    clf = DecisionTreeClassifier(max_depth=max_depth, criterion=criterion, random_state=21)\n",
    "    \n",
    "    # Si addestra il modello DecisionTreeClassifier sul train set\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Si salvano le predizioni effettuate\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Si salva l'accuratezza bilanciata del modello\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Si salvano i vari risultati d'interesse\n",
    "    result = {\n",
    "        \"techniques_list\": technique, \n",
    "        \"technique\": ', '.join(technique) if technique else 'no techniques',\n",
    "        \"accuracy\": balanced_accuracy,\n",
    "        \"max_depth\": max_depth,\n",
    "        \"criterion\": criterion\n",
    "    }\n",
    "    \n",
    "    # Si aggiungono ai risultati finali\n",
    "    result_decisionTrees.append(result)\n",
    "    \n",
    "    # Si definiscono le matrici binarie (1 vs All)\n",
    "    mcm = multilabel_confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Si salvano i nomi delle classi\n",
    "    class_names = clf.classes_\n",
    "    \n",
    "    # Si crea una figura con 3 righe e 3 colonne di subplot\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    \n",
    "    # Si inserisce il titolo personalizzato\n",
    "    fig.suptitle(f\"Correlation Matrix Decision Trees\\nTechniques: {', '.join(technique) if technique else 'no techniques'}\\nMax_depth: {max_depth}\\nCriterion: {criterion}\\nAccuracy: {balanced_accuracy}\")\n",
    "    \n",
    "    # Si trasforma la matrice 3x3 in un array 1D\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Si itera per ogni coppia matrice - classe corrispondente\n",
    "    for i, (matrix, class_name) in enumerate(zip(mcm, class_names)):\n",
    "        \n",
    "        # Si salvano i valori True Negative (tn) False Positive (fp)\n",
    "        # False Negative (fn) e True Positive (tp)\n",
    "        tn, fp = matrix[0]\n",
    "        fn, tp = matrix[1]\n",
    "\n",
    "        # Si calcola l'accuratezza per ogni classe\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        \n",
    "        # Si mostra il seguente messaggio nella matrice di correlazione\n",
    "        display_labels = [f\"Non-{class_name}\", f\"{class_name}\"]\n",
    "\n",
    "        # Si visualizza la matrice di ogni classe con ConfusionMatrixDisplay\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=display_labels)\n",
    "        disp.plot(ax=axes[i], cmap=\"Reds\")\n",
    "        axes[i].set_title(f\"{class_name} vs Resto\\nAccuracy: {accuracy}\")\n",
    "\n",
    "    # Si visualizza la matrice di confusione generale normalizzata \n",
    "    # si vedono le percentuali di corretta previsione e dove il modello fa confusione\n",
    "    cm = confusion_matrix(y_test, y_pred, normalize=\"true\")\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(ax=axes[7], cmap=\"Reds\", values_format=\".2f\")\n",
    "    axes[7].set_title(f\"Confusion Matrix Normalized\")\n",
    "    axes[7].set_xticklabels(class_names, rotation=90)\n",
    "    \n",
    "    # Non si visualizza l'ultimo subplot disponibile\n",
    "    axes[8].set_visible(False)\n",
    "    \n",
    "    # Si salva l'immagine nell'apposita cartella\n",
    "    plt.figure(fig.number)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{dir}/{'_'.join(technique) if technique else 'no_techniques'}_confusion_matrix\")\n",
    "    plt.close(fig)\n",
    "    \n",
    "# Si converte results in un pandas DataFrame e si ordinano i risultati\n",
    "result_decisionTrees = pd.DataFrame(result_decisionTrees)\n",
    "print(result_decisionTrees.sort_values(\"accuracy\", ascending=False))\n",
    "\n",
    "# Si ricava la migliore accuracy e si stamapa a video\n",
    "print(\"\\nMiglior configurazione:\")\n",
    "best_idx = result_decisionTrees['accuracy'].idxmax()\n",
    "best_config_decisionTree = result_decisionTrees.iloc[[best_idx]]\n",
    "print(best_config_decisionTree[[\"technique\", \"accuracy\", \"max_depth\", \"criterion\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Miglior configurazione:\n",
    "                    technique  accuracy  max_depth criterion\n",
    "standardization, oversampling  0.926019          9      gini\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si salva la miglior configurazione trovata\n",
    "best_configs_list.append((\"DecisionTree\", best_config_decisionTree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors\n",
    "\n",
    "L'algoritmo **k-NN** è un algoritmo di machine learning che definisce la classe di un punto sulla base delle classi delle **k osservazioni nelle vicinanze**. Viene definito come un algoritmo di *apprendimeto lazy* in quanto non costruisce un modello \"esplicito\" durante la fase di addestramento. Inoltre presenta una serie di metriche di distanza che si occupano di determinare come viene calcolata la similarità tra occorrenze.\n",
    "\n",
    "I punti di forza del k-NN sono la semplicità concettuale e implementativa, ma può diventare computazionalmente costoso con dataset di grandi dimensioni, inoltre ha la necessità che i dati appartengano alla stessa scala.\n",
    "\n",
    "La libreria *scikit-learn* mette a disposizione la classe [*KNeighborsClassifier*](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) per implementare l'algoritmo k-Nearest Neighbour. Quest'ultimo utilizza la distanza euclidea by default (come caso speciale della distanza minkowski con p = 2) e k = 5.\n",
    "\n",
    "Si effettua un tuning degli iperparametri tramite *GridSearchCV* per definire la miglior combinazione tra numero di k vicini da considerare, pesi per la distanza e distanza da utilizzare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# skf = StratifiedKFold è già stato definito precedentemente\n",
    "\n",
    "# Si definisce una lista per contenere i risultati\n",
    "results_knn = []\n",
    "\n",
    "# Si definisce il classificatore KNeighborsClassifier su cui effettuare\n",
    "# il tuning degli iperparametri. Essendo deterministico non presenta random state\n",
    "clf = KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "# Si definisce una lista dei vicini da testare durante la cross validation\n",
    "k_neighbors = list(range(1, 21)) # 1 - 20\n",
    "\n",
    "# Si definisce una lista dei parametri per la ponderazione da apllicare\n",
    "# ai vicini\n",
    "weights = ['uniform', 'distance']\n",
    "\n",
    "# Si definisce una lisa di metriche di distanze da utilizzare \n",
    "metric = ['euclidean', 'manhattan', 'chebyshev']\n",
    "\n",
    "# Si divide il dataset iniziale in train e test con stratificazione \n",
    "# ovvero le occorrenze delle classi sono automaticamente bilanciate\n",
    "# tra train e test (random state garantisce la riproducibilità)\n",
    "X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)\n",
    "\n",
    "# Si inizializzano le due variabili utilizzate successivamente per salvare il miglior\n",
    "# classificatore ttrovato tramite la grid search e cross validation\n",
    "best_knn_score = -1\n",
    "best_knn_clf = None\n",
    "\n",
    "# Si itera per tutte le combinazioni di tecniche di pre processing possibili \n",
    "# e precedentemente definite\n",
    "for technique in techniques:\n",
    "    \n",
    "    # Si definisce una lista per contenere i vari step per la cross validation\n",
    "    steps = []\n",
    "    \n",
    "    # Se la tecnica specificata tra virgolette è presente nella lista\n",
    "    # technique si aggiunge tra gli step della cross validation\n",
    "    if \"standardization\" in technique:\n",
    "        steps.append((\"scaler\", scaler))\n",
    "        \n",
    "        # Si salva il numero corretto di cluster per la standardizzazione \n",
    "        n_clusters = n_clusters_std\n",
    "        \n",
    "    elif \"normalization\" in technique:\n",
    "        steps.append((\"normalizer\", normalizer))\n",
    "        \n",
    "        # Si salva il numero corretto di cluster per la normalizzazione\n",
    "        n_clusters = n_clusters_std\n",
    "        \n",
    "    if \"agglomeration\" in technique:\n",
    "        \n",
    "        # Si definisce lo step dell'agglomerazione di feature con gli stessi \n",
    "        # parametri usati precedentemente \n",
    "        steps.append((\"agglomeration\", FeatureAgglomeration(\n",
    "            n_clusters=n_clusters,\n",
    "            metric=\"euclidean\",\n",
    "            linkage=\"ward\"\n",
    "        )))\n",
    "        \n",
    "    if \"undersampling\" in technique:\n",
    "        steps.append((\"undersampler\", cc))\n",
    "        \n",
    "    elif \"oversampling\" in technique:\n",
    "        steps.append((\"oversampler\", smote))\n",
    "    \n",
    "    # Si aggiunge il classificatore KNeighborsClassifier come ultimo step\n",
    "    # della cross validation\n",
    "    steps.append((\"classifier\", clf))\n",
    "    \n",
    "    # Si crea la pipeline di step \n",
    "    # si utilizza ImbPipeline invece di Pipeline in quanto accetta \n",
    "    # le tecniche di sampling (oversampling/undersampling)\n",
    "    pipeline = ImbPipeline(steps)\n",
    "    \n",
    "    # Si definisce il parametro di GridSearchCV per la ricerca degli\n",
    "    # iperparametri migliori\n",
    "    param_grid = {\n",
    "        \"classifier__n_neighbors\": k_neighbors,\n",
    "        \"classifier__weights\": weights,\n",
    "        \"classifier__metric\": metric\n",
    "    }\n",
    "    \n",
    "    # Si definisce GridSearchCV, la pipeline da seguire, i parametri,\n",
    "    # la tecnica per la cross validation (StratifiedKFold), lo scoring da \n",
    "    # utilizzare (balanced_accuracy in quanto il dataset è sbilanciato) e il\n",
    "    # numero di core da utilizzare contemporaneamente\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=skf,\n",
    "        scoring=\"balanced_accuracy\",\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Si esegue la cross validation sul train set, per ogni tecnica si \n",
    "    # verifica quale combinazione di iperparametri ottiene l'accuratezza maggiore\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Si verifica il risultato del modello, se migliore del precedente si salva il corrente\n",
    "    if grid_search.best_score_ > best_knn_score:\n",
    "        best_knn_score = grid_search.best_score_\n",
    "        best_knn_clf = grid_search.best_estimator_\n",
    "    \n",
    "    # Si estrae solo il miglior risultato per questa tecnica\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    # Si definiscono i risultati per ogni tecnica, profondità e criterio\n",
    "    result = {\n",
    "        \"techniques_list\": technique,                                   # Si mantiene la lista di tecniche utilizzate per comodità\n",
    "        \"techniques\": ', '.join(technique),                             # Si converte la lista di tecniche in stringa per migliore visualizzazione\n",
    "        \"n_neighbors\": best_params[\"classifier__n_neighbors\"],          # Si salva il valore di k\n",
    "        \"weights\": best_params[\"classifier__weights\"],                  # Si salva il parametro peso\n",
    "        \"metric\": best_params[\"classifier__metric\"],                    # Si salva la metrica\n",
    "        \"mean_test_score\": grid_search.best_score_,                     # Si salva il valore medio dell'accuratezza bilanciata ottenuto dalle 5 iterazioni della cross validation\n",
    "        \"std_test_score\": grid_search.cv_results_[\"std_test_score\"][    # Si salva la deviazione standard del miglior punteggio\n",
    "            grid_search.cv_results_[\"mean_test_score\"].argmax()\n",
    "        ]\n",
    "    }\n",
    "        \n",
    "    # Si aggiungono ai risultati finali\n",
    "    results_knn.append(result)\n",
    "    \n",
    "    # Si libera la memoria\n",
    "    del grid_search\n",
    "        \n",
    "# Si trasforma result in un pandas DataFrame\n",
    "results_knn = pd.DataFrame(results_knn)\n",
    "\n",
    "# Si restituisce la combinazione dei risultati migliori per ogni tecnica\n",
    "print(\"\\nMigliori configurazioni per ogni tecnica:\")\n",
    "print(results_knn[[\"techniques\", \"n_neighbors\", \"weights\", \"metric\", \"mean_test_score\", \"std_test_score\"]].sort_values(\"mean_test_score\", ascending=False)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Migliori configurazioni per ogni tecnica:\n",
    "                                   techniques  n_neighbors   weights     metric  mean_test_score  std_test_score\n",
    "                standardization, oversampling           14  distance  euclidean         0.935410        0.005006\n",
    "               standardization, undersampling            9  distance  manhattan         0.934222        0.002982\n",
    "                              standardization            6  distance  euclidean         0.933960        0.003315\n",
    "                 normalization, undersampling           12  distance  euclidean         0.931810        0.002194\n",
    "                                normalization            8  distance  euclidean         0.931211        0.002908\n",
    "                  normalization, oversampling           12  distance  chebyshev         0.931021        0.004047\n",
    "standardization, agglomeration, undersampling           14  distance  manhattan         0.905852        0.007083\n",
    " standardization, agglomeration, oversampling           18  distance  euclidean         0.905194        0.002247\n",
    "               standardization, agglomeration            8  distance  euclidean         0.902804        0.001674\n",
    "                 normalization, agglomeration           17   uniform  euclidean         0.897193        0.004668\n",
    "  normalization, agglomeration, undersampling           19  distance  manhattan         0.897127        0.007467\n",
    "   normalization, agglomeration, oversampling           19  distance  manhattan         0.895566        0.004555\n",
    "                                 oversampling            6  distance  manhattan         0.786889        0.004193\n",
    "                                                         7  distance  manhattan         0.786682        0.005773\n",
    "                                undersampling            4  distance  manhattan         0.784426        0.012981\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_knn_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si verificano i risultati del codice precedente:\n",
    "- Per ogni configurazione trovata, si applicano le medesime tecniche di preprocessing e si addestra un algoritmo KNN con gli ipeparametri ottimali.\n",
    "- Si esegue una valutazione delle performance sul test set.\n",
    "- Si visualizzano le matrici di confusione per ogni configurazione.\n",
    "- Si salvano i risultati della migliore configurazione. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si crea la directory per KNN se non esiste\n",
    "dir = \"6 - KNN\"\n",
    "path_dir = os.path.join(os.getcwd(), dir)\n",
    "\n",
    "if not os.path.exists(path_dir):\n",
    "    os.makedirs(path_dir)\n",
    "\n",
    "# Si crea una lista per contenere i vari risultati\n",
    "result_knn = []\n",
    "\n",
    "# Si itera lungo le righe delle configurazioni migliori \n",
    "for row in results_knn.itertuples():\n",
    "    \n",
    "    # Si salva la lista contenente le tecniche di pre processing \n",
    "    technique = row.techniques_list\n",
    "    \n",
    "    # Si divide il dataset iniziale in train e test con stratificazione\n",
    "    # random_state = 21 ci garantisce che per ogni iterazione otteniamo \n",
    "    # sempre le stesse separazioni e si combinano le varie tecniche di preprocessing\n",
    "    X_train, X_test, y_train, y_test = combine_preprocessing(X, y, technique)\n",
    "    \n",
    "    # Si salva il valore di n (vicini) \n",
    "    k_neighbors = row.n_neighbors\n",
    "    \n",
    "    # Si salva il parametro peso \n",
    "    weights = row.weights\n",
    "    \n",
    "    # Si salva la metrica considerata migliore\n",
    "    metric = row.metric\n",
    "    \n",
    "    # Si definisce KNeighborsClassifier con gli iperparametri definiti\n",
    "    clf = KNeighborsClassifier(n_neighbors=k_neighbors, weights=weights, metric=metric, n_jobs=-1)\n",
    "    \n",
    "    # Si addestra il modello KNeighborsClassifier sul train set\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Si salvano le predizioni effettuate\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Si salva l'accuratezza bilanciata del modello\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Si salvano i vari risultati d'interesse\n",
    "    result = {\n",
    "        \"techniques_list\": technique, \n",
    "        \"technique\": ', '.join(technique) if technique else 'no techniques',\n",
    "        \"accuracy\": balanced_accuracy,\n",
    "        \"k_neighbors\": k_neighbors,\n",
    "        \"weights\": weights,\n",
    "        \"metric\": metric\n",
    "    }\n",
    "    \n",
    "    # Si aggiungono ai risultati finali\n",
    "    result_knn.append(result)\n",
    "    \n",
    "    # Si definiscono le matrici binarie (1 vs All)\n",
    "    mcm = multilabel_confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Si salvano i nomi delle classi\n",
    "    class_names = clf.classes_\n",
    "    \n",
    "    # Si crea una figura con 3 righe e 3 colonne di subplot\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    \n",
    "    # Si inserisce il titolo personalizzato\n",
    "    fig.suptitle(f\"Correlation Matrix KNN\\nTechniques: {', '.join(technique) if technique else 'no techniques'}\\nK Neighbors: {k_neighbors}\\nMetric: {metric} with {weights} weights\\nAccuracy: {balanced_accuracy}\")\n",
    "    \n",
    "    # Si trasforma la matrice 3x3 in un array 1D\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Si itera per ogni coppia matrice - classe corrispondente\n",
    "    for i, (matrix, class_name) in enumerate(zip(mcm, class_names)):\n",
    "        \n",
    "        # Si salvano i valori True Negative (tn) False Positive (fp)\n",
    "        # False Negative (fn) e True Positive (tp)\n",
    "        tn, fp = matrix[0]\n",
    "        fn, tp = matrix[1]\n",
    "\n",
    "        # Si calcola l'accuratezza per ogni classe\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        \n",
    "        # Si mostra il seguente messaggio nella matrice di correlazione\n",
    "        display_labels = [f\"Non-{class_name}\", f\"{class_name}\"]\n",
    "\n",
    "        # Si visualizza la matrice di ogni classe con ConfusionMatrixDisplay\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=display_labels)\n",
    "        disp.plot(ax=axes[i], cmap=\"Reds\")\n",
    "        axes[i].set_title(f\"{class_name} vs Resto\\nAccuracy: {accuracy}\")\n",
    "\n",
    "    # Si visualizza la matrice di confusione generale normalizzata \n",
    "    # si vedono le percentuali di corretta previsione e dove il modello fa confusione\n",
    "    cm = confusion_matrix(y_test, y_pred, normalize=\"true\")\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(ax=axes[7], cmap=\"Reds\", values_format=\".2f\")\n",
    "    axes[7].set_title(f\"Confusion Matrix Normalized\")\n",
    "    axes[7].set_xticklabels(class_names, rotation=90)\n",
    "    \n",
    "    # Non si visualizza l'ultimo subplot disponibile\n",
    "    axes[8].set_visible(False)\n",
    "    \n",
    "    # Si salva l'immagine nell'apposita cartella\n",
    "    plt.figure(fig.number)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{dir}/{'_'.join(technique) if technique else 'no_techniques'}_confusion_matrix\")\n",
    "    plt.close(fig)\n",
    "    \n",
    "# Si converte results in un pandas DataFrame e si ordinano i risultati\n",
    "result_knn = pd.DataFrame(result_knn)\n",
    "print(result_knn.sort_values(\"accuracy\", ascending=False))\n",
    "\n",
    "# Si ricava la migliore accuracy e si stamapa a video\n",
    "print(\"\\nMiglior configurazione:\")\n",
    "best_idx = result_knn['accuracy'].idxmax()\n",
    "best_config_knn = result_knn.iloc[[best_idx]]\n",
    "print(best_config_knn[[\"technique\", \"accuracy\", \"k_neighbors\", \"weights\", \"metric\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Miglior configurazione:\n",
    "                    technique  accuracy  k_neighbors   weights     metric\n",
    "standardization, oversampling  0.936496           14  distance  euclidean\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si salva la miglior configurazione trovata\n",
    "best_configs_list.append((\"KNN\", best_config_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "I Naive Bayes sono una famiglia di algoritmi di classificazione probabilistici basati sul **teorema di Bayes** con assunzione di indipendenza tra le feature. Questi algoritmi calcolano la *probabilità di appartenenza a ciascuna classe* e assegnano l'osservazione alla classe con la *probabilità più elevata*.\n",
    "\n",
    "L'assunzione di indipendenza condizionale tra le feature (anche se spesso è una semplificazione della realtà) rende questi algoritmi computazionalmente efficienti.\n",
    "\n",
    "I punti di frza dei classificatori Naive Bayes sono la semplicità concettuale, la velocità di addestramento e predizione, la robustezza al rumore e la capacità di gestire dati ad alta dimensionalità. \n",
    "\n",
    "La libreria *scikit-learn* implementa diverse varianti di Naive Bayes, ciascuna adatta a diverse distribuzioni di dati. Tra queste, **GaussianNB** è specifica per feature continue che si assume seguano una distribuzione normale all'interno di ciascuna classe. [*GaussianNB*](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) stima media e varianza per ogni feature condizionata alla classe, e utilizza la funzione di densità di probabilità gaussiana per calcolare le probabilità durante la fase predittiva.\n",
    "\n",
    "Si effettua un tuning degli iperparametri tramite *GridSearchCV* per definire la miglior *var_smoothing* (per stabilizzare il calcolo). Inoltre si verifica se i risultati migliorano impostando *priors* (ovvero le probabilità a priori delle classi) con valori inversi alla frequenza delle classi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# skf = StratifiedKFold è già stato definito precedentemente\n",
    "\n",
    "# Si definisce una lista per contenere i risultati\n",
    "results_naiveBayes = []\n",
    "\n",
    "# Si definisce il classificatore GaussianNB su cui effettuare\n",
    "# il tuning degli iperparametri. Essendo deterministico non presenta random state\n",
    "clf = GaussianNB()\n",
    "\n",
    "# Si definisce una lista delle var_smoothing da testare (scala logaritmica, usiamo logspace)\n",
    "var_smoothing = np.logspace(-17, -9, 9) # [1.e-17, 1.e-16, 1.e-15, 1.e-14, 1.e-13, 1.e-12, 1.e-11, 1.e-10, 1.e-09]7]\n",
    "\n",
    "# Si divide il dataset iniziale in train e test con stratificazione \n",
    "# ovvero le occorrenze delle classi sono automaticamente bilanciate\n",
    "# tra train e test (random state garantisce la riproducibilità)\n",
    "X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)\n",
    "\n",
    "# Si calcolano i pesi inversamente proporzionali\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "# Si normalizzano per ottenere probabilità che sommano a 1\n",
    "weights = class_weights / class_weights.sum()\n",
    "\n",
    "# Si definisce una lista delle probabilità a priori da testare nel modello\n",
    "priors = [None, weights]\n",
    "\n",
    "# Si inizializzano le due variabili utilizzate successivamente per salvare il miglior\n",
    "# classificatore ttrovato tramite la grid search e cross validation\n",
    "best_naiveBayes_score = -1\n",
    "best_naiveBayes_clf = None\n",
    "\n",
    "# Si itera per tutte le combinazioni di tecniche di pre processing possibili \n",
    "# e precedentemente definite\n",
    "for technique in techniques:\n",
    "    \n",
    "    # Si definisce una lista per contenere i vari step per la cross validation\n",
    "    steps = []\n",
    "    \n",
    "    # Se la tecnica specificata tra virgolette è presente nella lista\n",
    "    # technique si aggiunge tra gli step della cross validation\n",
    "    if \"standardization\" in technique:\n",
    "        steps.append((\"scaler\", scaler))\n",
    "        \n",
    "        # Si salva il numero corretto di cluster per la standardizzazione \n",
    "        n_clusters = n_clusters_std\n",
    "        \n",
    "    elif \"normalization\" in technique:\n",
    "        steps.append((\"normalizer\", normalizer))\n",
    "        \n",
    "        # Si salva il numero corretto di cluster per la normalizzazione\n",
    "        n_clusters = n_clusters_std\n",
    "        \n",
    "    if \"agglomeration\" in technique:\n",
    "        \n",
    "        # Si definisce lo step dell'agglomerazione di feature con gli stessi \n",
    "        # parametri usati precedentemente \n",
    "        steps.append((\"agglomeration\", FeatureAgglomeration(\n",
    "            n_clusters=n_clusters,\n",
    "            metric=\"euclidean\",\n",
    "            linkage=\"ward\"\n",
    "        )))\n",
    "        \n",
    "    if \"undersampling\" in technique:\n",
    "        steps.append((\"undersampler\", cc))\n",
    "        \n",
    "    elif \"oversampling\" in technique:\n",
    "        steps.append((\"oversampler\", smote))\n",
    "    \n",
    "    # Si aggiunge il classificatore KNeighborsClassifier come ultimo step\n",
    "    # della cross validation\n",
    "    steps.append((\"classifier\", clf))\n",
    "    \n",
    "    # Si crea la pipeline di step \n",
    "    # si utilizza ImbPipeline invece di Pipeline in quanto accetta \n",
    "    # le tecniche di sampling (oversampling/undersampling)\n",
    "    pipeline = ImbPipeline(steps)\n",
    "    \n",
    "    # Si definisce il parametro di GridSearchCV per la ricerca degli\n",
    "    # iperparametri migliori\n",
    "    param_grid = {\n",
    "        \"classifier__var_smoothing\": var_smoothing,\n",
    "        \"classifier__priors\": priors\n",
    "    }\n",
    "    \n",
    "    # Si definisce GridSearchCV, la pipeline da seguire, i parametri,\n",
    "    # la tecnica per la cross validation (StratifiedKFold), lo scoring da \n",
    "    # utilizzare (balanced_accuracy in quanto il dataset è sbilanciato) e il\n",
    "    # numero di core da utilizzare contemporaneamente\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=skf,\n",
    "        scoring=\"balanced_accuracy\",\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Si esegue la cross validation sul train set, per ogni tecnica si \n",
    "    # verifica quale combinazione di iperparametri ottiene l'accuratezza maggiore\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Si verifica il risultato del modello, se migliore del precedente si salva il corrente\n",
    "    if grid_search.best_score_ > best_naiveBayes_score:\n",
    "        best_naiveBayes_score = grid_search.best_score_\n",
    "        best_naiveBayes_clf = grid_search.best_estimator_\n",
    "    \n",
    "    # Si estrae solo il miglior risultato per questa tecnica\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    # Si definiscono i risultati per ogni tecnica, profondità e criterio\n",
    "    result = {\n",
    "        \"techniques_list\": technique,                                   # Si mantiene la lista di tecniche utilizzate per comodità\n",
    "        \"techniques\": ', '.join(technique),                             # Si converte la lista di tecniche in stringa per migliore visualizzazione\n",
    "        \"var_smoothing\": best_params[\"classifier__var_smoothing\"],      # Si salva il valore di var_smoothing\n",
    "        \"priors\": best_params[\"classifier__priors\"],                    # Si salva il parametro probabilità a priori\n",
    "        \"mean_test_score\": grid_search.best_score_,                     # Si salva il valore medio dell'accuratezza bilanciata ottenuto dalle 5 iterazioni della cross validation\n",
    "        \"std_test_score\": grid_search.cv_results_[\"std_test_score\"][    # Si salva la deviazione standard del miglior punteggio\n",
    "            grid_search.cv_results_[\"mean_test_score\"].argmax()\n",
    "        ]\n",
    "    }\n",
    "        \n",
    "    # Si aggiungono ai risultati finali\n",
    "    results_naiveBayes.append(result)\n",
    "\n",
    "    # Si libera la memoria\n",
    "    del grid_search\n",
    "    \n",
    "# Si trasforma result in un pandas DataFrame\n",
    "results_naiveBayes = pd.DataFrame(results_naiveBayes)\n",
    "\n",
    "# Si restituisce la combinazione dei risultati migliori per ogni tecnica\n",
    "print(\"\\nMigliori configurazioni per ogni tecnica:\")\n",
    "print(results_naiveBayes[[\"techniques\", \"var_smoothing\", \"priors\", \"mean_test_score\", \"std_test_score\"]].sort_values(\"mean_test_score\", ascending=False)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Migliori configurazioni per ogni tecnica:\n",
    "                                   techniques  var_smoothing    priors  mean_test_score  std_test_score\n",
    "                                 oversampling   1.000000e-16   Weights         0.911079        0.004048\n",
    "                                                1.000000e-15      None         0.909188        0.003429\n",
    "                  normalization, oversampling   1.000000e-17   Weights         0.908160        0.002980\n",
    "                standardization, oversampling   1.000000e-17   Weights         0.907771        0.002996\n",
    "                              standardization   1.000000e-17      None         0.907439        0.003649\n",
    "                                normalization   1.000000e-17      None         0.907439        0.003649\n",
    "                                undersampling   1.000000e-17      None         0.904302        0.002789\n",
    "                 normalization, undersampling   1.000000e-17   Weights         0.902195        0.004325\n",
    "               standardization, undersampling   1.000000e-17   Weights         0.902069        0.004143\n",
    " standardization, agglomeration, oversampling   1.000000e-17   Weights         0.882559        0.005692\n",
    "               standardization, agglomeration   1.000000e-17   Weights         0.881982        0.005774\n",
    "                 normalization, agglomeration   1.000000e-17      None         0.881065        0.004722\n",
    "   normalization, agglomeration, oversampling   1.000000e-17   Weights         0.880719        0.005252\n",
    "standardization, agglomeration, undersampling   1.000000e-17      None         0.878343        0.005376\n",
    "  normalization, agglomeration, undersampling   1.000000e-17   Weights         0.878152        0.006201\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_naiveBayes_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si verificano i risultati del codice precedente:\n",
    "- Per ogni configurazione trovata, si applicano le medesime tecniche di preprocessing e si addestra un algoritmo NaiveBayes (GaussianNB) con gli ipeparametri ottimali.\n",
    "- Si esegue una valutazione delle performance sul test set.\n",
    "- Si visualizzano le matrici di confusione per ogni configurazione.\n",
    "- Si salvano i risultati della migliore configurazione. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si crea la directory per la Naive Bayes Gaussiana se non esiste\n",
    "dir = \"7 - NAIVE BAYES\"\n",
    "path_dir = os.path.join(os.getcwd(), dir)\n",
    "\n",
    "if not os.path.exists(path_dir):\n",
    "    os.makedirs(path_dir)\n",
    "\n",
    "# Si crea una lista per contenere i vari risultati\n",
    "result_naiveBayes = []\n",
    "\n",
    "# Si itera lungo le righe delle configurazioni migliori \n",
    "for row in results_naiveBayes.itertuples():\n",
    "    \n",
    "    # Si salva la lista contenente le tecniche di pre processing \n",
    "    technique = row.techniques_list\n",
    "    \n",
    "    # Si divide il dataset iniziale in train e test con stratificazione\n",
    "    # random_state = 21 ci garantisce che per ogni iterazione otteniamo \n",
    "    # sempre le stesse separazioni e si combinano le varie tecniche di preprocessing\n",
    "    X_train, X_test, y_train, y_test = combine_preprocessing(X, y, technique)\n",
    "    \n",
    "    # Si salva il valore di var_smoothing\n",
    "    var_smoothing = row.var_smoothing\n",
    "    \n",
    "    # Si salva il parametro priors\n",
    "    priors = row.priors\n",
    "    \n",
    "    # Si definisce GaussianNB con gli iperparametri definiti\n",
    "    clf = GaussianNB(var_smoothing=var_smoothing, priors=priors)\n",
    "    \n",
    "    # Si addestra il modello GaussianNB sul train set\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Si salvano le predizioni effettuate\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Si salva l'accuratezza bilanciata del modello\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Si salvano i vari risultati d'interesse\n",
    "    result = {\n",
    "        \"techniques_list\": technique, \n",
    "        \"technique\": ', '.join(technique) if technique else 'no techniques',\n",
    "        \"accuracy\": balanced_accuracy,\n",
    "        \"var_smoothing\": var_smoothing,\n",
    "        \"priors\": priors\n",
    "    }\n",
    "    \n",
    "    # Si aggiungono ai risultati finali\n",
    "    result_naiveBayes.append(result)\n",
    "    \n",
    "    # Si definiscono le matrici binarie (1 vs All)\n",
    "    mcm = multilabel_confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Si salvano i nomi delle classi\n",
    "    class_names = clf.classes_\n",
    "    \n",
    "    # Si crea una figura con 3 righe e 3 colonne di subplot\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    \n",
    "    # Si inserisce il titolo personalizzato\n",
    "    fig.suptitle(f\"Correlation Matrix NaiveBayes\\nTechniques: {', '.join(technique) if technique else 'no techniques'}\\nVar Smoothing: {var_smoothing}\\nPriors: {priors}\\nAccuracy: {balanced_accuracy}\")\n",
    "    \n",
    "    # Si trasforma la matrice 3x3 in un array 1D\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Si itera per ogni coppia matrice - classe corrispondente\n",
    "    for i, (matrix, class_name) in enumerate(zip(mcm, class_names)):\n",
    "        \n",
    "        # Si salvano i valori True Negative (tn) False Positive (fp)\n",
    "        # False Negative (fn) e True Positive (tp)\n",
    "        tn, fp = matrix[0]\n",
    "        fn, tp = matrix[1]\n",
    "\n",
    "        # Si calcola l'accuratezza per ogni classe\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        \n",
    "        # Si mostra il seguente messaggio nella matrice di correlazione\n",
    "        display_labels = [f\"Non-{class_name}\", f\"{class_name}\"]\n",
    "\n",
    "        # Si visualizza la matrice di ogni classe con ConfusionMatrixDisplay\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=display_labels)\n",
    "        disp.plot(ax=axes[i], cmap=\"Reds\")\n",
    "        axes[i].set_title(f\"{class_name} vs Resto\\nAccuracy: {accuracy}\")\n",
    "\n",
    "    # Si visualizza la matrice di confusione generale normalizzata \n",
    "    # si vedono le percentuali di corretta previsione e dove il modello fa confusione\n",
    "    cm = confusion_matrix(y_test, y_pred, normalize=\"true\")\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(ax=axes[7], cmap=\"Reds\", values_format=\".2f\")\n",
    "    axes[7].set_title(f\"Confusion Matrix Normalized\")\n",
    "    axes[7].set_xticklabels(class_names, rotation=90)\n",
    "    \n",
    "    # Non si visualizza l'ultimo subplot disponibile\n",
    "    axes[8].set_visible(False)\n",
    "    \n",
    "    # Si salva l'immagine nell'apposita cartella\n",
    "    plt.figure(fig.number)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{dir}/{'_'.join(technique) if technique else 'no_techniques'}_confusion_matrix\")\n",
    "    plt.close(fig)\n",
    "    \n",
    "# Si converte results in un pandas DataFrame e si ordinano i risultati\n",
    "result_naiveBayes = pd.DataFrame(result_naiveBayes)\n",
    "print(result_naiveBayes.sort_values(\"accuracy\", ascending=False))\n",
    "\n",
    "# Si ricava la migliore accuracy e si stamapa a video\n",
    "print(\"\\nMiglior configurazione:\")\n",
    "best_idx = result_naiveBayes['accuracy'].idxmax()\n",
    "best_config_naiveBayes = result_naiveBayes.iloc[[best_idx]]\n",
    "print(best_config_naiveBayes[[\"technique\", \"accuracy\", \"var_smoothing\", \"priors\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Miglior configurazione:\n",
    "   technique  accuracy  var_smoothing  priors\n",
    "oversampling   0.90995   1.000000e-16 Weights\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si salva la miglior configurazione trovata\n",
    "best_configs_list.append((\"NaiveBayes\", best_config_naiveBayes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier (SVC)\n",
    "\n",
    "SVC è un algoritmo di machine learning che cerca di trovare un *iperpiano ottimale per separare dati appartenenti a classi diverse*. Opera massimizzando il margine tra le classi, ovvero la distanza tra l'iperpiano di separazione e i punti più vicini di ciascuna classe (chiamati vettori di supporto).\n",
    "\n",
    "L'algoritmo utilizza il \"kernel trick\" che permette di trasformare implicitamente i dati in uno spazio dimensionale superiore, dove potrebbero diventare linearmente separabili. SVC supporta diversi tipi di kernel.\n",
    "\n",
    "Tra i punti di forza di SVC c'è l'efficacia in spazi ad alta dimensionalità, tuttavia, non scala bene con dataset molto grandi e richiede un'attenta ottimizzazione degli iperparametri.\n",
    "\n",
    "La libreria *scikit-learn* implementa la classe [*SVC*](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html). By default utilizza kernel RBF con gamma='scale' e C=1.0, rendendo il modello abbastanza flessibile ma suscettibile a overfitting in alcuni casi.\n",
    "\n",
    "Si effettua un tuning degli iperparametri tramite *GridSearchCV* per trovare la migliore combinazione di:\n",
    "- **Kernel** (*linear* o *rbf*): funzione che trasforma i dati in uno spazio dove diventano più facilmente separabili.\n",
    "- **C**: definisce quanto il modello deve evitare errori di classificazione. \n",
    "- **Gamma**: determina quanto lontano arriva l'influenza di ogni punto.\n",
    "- **Class weight** (*balaced*): assegna l'importanza agli errori di ciascuna classe (utile in caso di dataset sbilanciato).\n",
    "\n",
    "Si noti che, essendo *SVC* una tecnica molto costosa computazionalmente e dal punto di vista della gestione della memoria, si effettua un tuning degli iperparametri tramite *GridSearchCV* su un subset del dataset (10% del dataset originale con startificazione)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Si definisce una lista per contenere i risultati\n",
    "results_svc = []\n",
    "\n",
    "# Si definisce il classificatore SVC su cui effettuare il tuning degli iperparametri.\n",
    "# Si imposta random state per la riproducibilità\n",
    "clf = SVC(random_state=21)\n",
    "\n",
    "# Si definisce una lista dei parametri di C da testare \n",
    "# tipicamente su scale logaritmiche, si usa logspace di numpy\n",
    "c = np.logspace(-1, 1, 3) # [ 0.1, 1, 10]\n",
    "\n",
    "# Si definisce una lista dei parametri di gamma da testare\n",
    "# tipicamente su scale logaritmiche, si usa logspace di numpy\n",
    "gamma = ['scale', 'auto'] + list(np.logspace(-1, 0, 2)) # ['scale', \"auto\", 0.1 , 1]\n",
    "\n",
    "# Si definisce la metrica per la valutazione dei pesi associati alle classi\n",
    "class_weight = [None, 'balanced']\n",
    "\n",
    "# Si definiscono grid diversi per kernel diversi (in quanto solo rbf considera gamma come parametro)\n",
    "linear_params = {\n",
    "    \"classifier__kernel\": ['linear'], \n",
    "    \"classifier__C\": c, \n",
    "    \"classifier__class_weight\": class_weight\n",
    "}\n",
    "rbf_params = {\n",
    "    \"classifier__kernel\": ['rbf'], \n",
    "    \"classifier__C\": c, \n",
    "    \"classifier__gamma\": gamma,\n",
    "    \"classifier__class_weight\": class_weight\n",
    "    }\n",
    "\n",
    "# Si definisce un subset del dataset originale\n",
    "X_subset, _, y_subset, _ = train_test_split(X, y, train_size=0.1, random_state=21, stratify=y)\n",
    "\n",
    "# Si divide il subset del dataset originale in train e test con stratificazione \n",
    "# ovvero le occorrenze delle classi sono automaticamente bilanciate\n",
    "# tra train e test (random state garantisce la riproducibilità)\n",
    "X_train, _, y_train, _ = train_test_split(X_subset, y_subset, test_size=0.2, random_state=21, stratify=y_subset)\n",
    "\n",
    "# Si inizializzano le due variabili utilizzate successivamente per salvare il miglior\n",
    "# classificatore ttrovato tramite la grid search e cross validation\n",
    "best_svc_score = -1\n",
    "best_svc_clf = None\n",
    "\n",
    "# Si itera per tutte le combinazioni di tecniche di pre processing possibili \n",
    "# e precedentemente definite\n",
    "for technique in techniques:\n",
    "    \n",
    "    # Si definisce una lista per contenere i vari step per la cross validation\n",
    "    steps = []\n",
    "    \n",
    "    # Se la tecnica specificata tra virgolette è presente nella lista\n",
    "    # technique si aggiunge tra gli step della cross validation\n",
    "    if \"standardization\" in technique:\n",
    "        steps.append((\"scaler\", scaler))\n",
    "        \n",
    "        # Si salva il numero corretto di cluster per la standardizzazione \n",
    "        n_clusters = n_clusters_std\n",
    "        \n",
    "    elif \"normalization\" in technique:\n",
    "        steps.append((\"normalizer\", normalizer))\n",
    "        \n",
    "        # Si salva il numero corretto di cluster per la normalizzazione\n",
    "        n_clusters = n_clusters_std\n",
    "        \n",
    "    if \"agglomeration\" in technique:\n",
    "        \n",
    "        # Si definisce lo step dell'agglomerazione di feature con gli stessi \n",
    "        # parametri usati precedentemente \n",
    "        steps.append((\"agglomeration\", FeatureAgglomeration(\n",
    "            n_clusters=n_clusters,\n",
    "            metric=\"euclidean\",\n",
    "            linkage=\"ward\"\n",
    "        )))\n",
    "        \n",
    "    if \"undersampling\" in technique:\n",
    "        steps.append((\"undersampler\", cc))\n",
    "        \n",
    "    elif \"oversampling\" in technique:\n",
    "        steps.append((\"oversampler\", smote))\n",
    "    \n",
    "    # Si aggiunge il classificatore SVC come ultimo step della cross validation\n",
    "    steps.append((\"classifier\", clf))\n",
    "    \n",
    "    # Si crea la pipeline di step \n",
    "    # si utilizza ImbPipeline invece di Pipeline in quanto accetta \n",
    "    # le tecniche di sampling (oversampling/undersampling)\n",
    "    pipeline = ImbPipeline(steps)\n",
    "\n",
    "    # Si definisce il parametro param_grid come combinazione degli iperparametri da valutare\n",
    "    param_grid = [linear_params, rbf_params]\n",
    "    \n",
    "    # Si definisce GridSearchCV, la pipeline da seguire, i parametri,\n",
    "    # la tecnica per la cross validation (StratifiedKFold), lo scoring da \n",
    "    # utilizzare (balanced_accuracy in quanto il dataset è sbilanciato) e il\n",
    "    # numero di core da utilizzare contemporaneamente\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=skf,\n",
    "        scoring=\"balanced_accuracy\",\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Si esegue la cross validation sul train set, per ogni tecnica si \n",
    "    # verifica quale combinazione di iperparametri ottiene l'accuratezza maggiore\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Si verifica il risultato del modello, se migliore del precedente si salva il corrente\n",
    "    if grid_search.best_score_ > best_svc_score:\n",
    "        best_svc_score = grid_search.best_score_\n",
    "        best_svc_clf = grid_search.best_estimator_\n",
    "        \n",
    "    # Si estrae solo il miglior risultato per questa tecnica\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    # Si crea il dizionario con i risultati migliori per questa tecnica\n",
    "    result = {\n",
    "        \"techniques_list\": technique,                                        # Si mantiene la lista di tecniche utilizzate per comodità\n",
    "        \"techniques\": ', '.join(technique),                                  # SI converte in stringa la lista delle tecniche utilizzate\n",
    "        \"kernel\": best_params[\"classifier__kernel\"],                         # Si salva il Kernel migliore\n",
    "        \"C\": best_params[\"classifier__C\"],                                   # Si salva il parametro C migliore\n",
    "        \"gamma\": best_params.get(\"classifier__gamma\", \"None\"),               # Si salva il parametro Gamma migliore (se presente)\n",
    "        \"class_weight\": best_params[\"classifier__class_weight\"],             # Si salva il Class weight migliore\n",
    "        \"mean_test_score\": grid_search.best_score_,                          # Si salva lo score\n",
    "        \"std_test_score\": grid_search.cv_results_[\"std_test_score\"][         # Si salva la deviazione standard del miglior punteggio\n",
    "            grid_search.cv_results_[\"mean_test_score\"].argmax()\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Si aggiunge solo il miglior risultato ai risultati finali\n",
    "    results_svc.append(result)\n",
    "    \n",
    "    # Si libera la memoria\n",
    "    del grid_search\n",
    "    \n",
    "# Si trasforma results in un pandas DataFrame\n",
    "results_svc = pd.DataFrame(results_svc)\n",
    "\n",
    "# Si restituiscono i risultati ordinati per performance\n",
    "print(\"\\nMigliori configurazioni per ogni tecnica:\")\n",
    "print(results_svc[[\"techniques\", \"kernel\", \"C\", \"gamma\", \"class_weight\", \"mean_test_score\", \"std_test_score\"]].sort_values(\"mean_test_score\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Migliori configurazioni per ogni tecnica:\n",
    "                                   techniques  kernel     C  gamma  class_weight  mean_test_score  std_test_score\n",
    "                 normalization, undersampling     rbf  10.0  scale          None         0.949491        0.010041\n",
    "                              standardization     rbf  10.0   0.01          None         0.948215        0.008798\n",
    "                                normalization     rbf  10.0  scale          None         0.947795        0.010418\n",
    "                  normalization, oversampling     rbf   1.0  scale          None         0.946671        0.005631\n",
    "                standardization, oversampling  linear   0.1   None          None         0.946648        0.005566\n",
    "               standardization, agglomeration  linear  10.0   None      balanced         0.945868        0.004841\n",
    "               standardization, undersampling     rbf  10.0   0.01          None         0.945500        0.008319\n",
    " standardization, agglomeration, oversampling  linear  10.0   None          None         0.943577        0.003275\n",
    "                                               linear   1.0   None          None         0.937080        0.006882\n",
    "                                 oversampling  linear   1.0   None          None         0.935499        0.011211\n",
    "standardization, agglomeration, undersampling     rbf  10.0  scale          None         0.930183        0.005209\n",
    "   normalization, agglomeration, oversampling     rbf  10.0    1.0          None         0.919025        0.009025\n",
    "                 normalization, agglomeration     rbf  10.0  scale      balanced         0.916079        0.008972\n",
    "  normalization, agglomeration, undersampling     rbf  10.0  scale          None         0.913489        0.012633\n",
    "                                undersampling  linear  10.0   None          None         0.910412        0.013463\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_svc_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si verificano i risultati del codice precedente:\n",
    "- Per ogni configurazione trovata, si applicano le medesime tecniche di preprocessing e si addestra un algoritmo SVC con gli ipeparametri ottimali.\n",
    "- Si esegue una valutazione delle performance sul test set.\n",
    "- Si visualizzano le matrici di confusione per ogni configurazione.\n",
    "- Si salvano i risultati della migliore configurazione. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si crea la directory per SVC se non esiste\n",
    "dir = \"8 - SVC\"\n",
    "path_dir = os.path.join(os.getcwd(), dir)\n",
    "\n",
    "if not os.path.exists(path_dir):\n",
    "    os.makedirs(path_dir)\n",
    "\n",
    "# Si crea una lista per contenere i vari risultati\n",
    "result_svc = []\n",
    "\n",
    "# Si ridefinisce il medesimo subset per lavorare con il test set corretto\n",
    "X_subset, _, y_subset, _ = train_test_split(X, y, train_size=0.1, random_state=21, stratify=y)\n",
    "\n",
    "# Si itera lungo le righe delle configurazioni migliori \n",
    "for row in results_svc.itertuples():\n",
    "    \n",
    "    # Si salva la lista contenente le tecniche di pre processing \n",
    "    technique = row.techniques_list\n",
    "    \n",
    "    # Si divide il subset del dataset in train e test con stratificazione\n",
    "    # random_state = 21 ci garantisce che per ogni iterazione otteniamo \n",
    "    # sempre le stesse separazioni e si combinano le varie tecniche di preprocessing\n",
    "    X_train, X_test, y_train, y_test = combine_preprocessing(X_subset, y_subset, technique)\n",
    "    \n",
    "    # Si salva il valore d C\n",
    "    c = row.C\n",
    "    \n",
    "    # Si salva il parametro gamma \n",
    "    gamma = row.gamma\n",
    "    \n",
    "    # Si salva il parametro peso\n",
    "    class_weight = row.class_weight\n",
    "    \n",
    "    # Si salva il kernel utilizzato\n",
    "    kernel = row.kernel\n",
    "    \n",
    "    # Si definisce SVC con gli iperparametri definiti\n",
    "    if gamma == \"None\":\n",
    "        clf = SVC(C=c, kernel=kernel, class_weight=class_weight, random_state=21)\n",
    "    else:\n",
    "        clf = SVC(C=c,kernel=kernel, gamma=gamma, class_weight=class_weight, random_state=21)\n",
    "    \n",
    "    # Si addestra il modello KNeighborsClassifier sul train set\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Si salvano le predizioni effettuate\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Si salva l'accuratezza bilanciata del modello\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Si salvano i vari risultati d'interesse\n",
    "    result = {\n",
    "        \"techniques_list\": technique, \n",
    "        \"technique\": ', '.join(technique) if technique else 'no techniques',\n",
    "        \"accuracy\": balanced_accuracy,\n",
    "        \"kernel\": kernel,\n",
    "        \"C\": c,\n",
    "        \"gamma\": gamma,\n",
    "        \"class_weight\": class_weight\n",
    "    }\n",
    "    \n",
    "    # Si aggiungono ai risultati finali\n",
    "    result_svc.append(result)\n",
    "    \n",
    "    # Si definiscono le matrici binarie (1 vs All)\n",
    "    mcm = multilabel_confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Si salvano i nomi delle classi\n",
    "    class_names = clf.classes_\n",
    "    \n",
    "    # Si crea una figura con 3 righe e 3 colonne di subplot\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    \n",
    "    # Si inserisce il titolo personalizzato\n",
    "    if gamma == \"None\":\n",
    "        fig.suptitle(f\"Correlation Matrix SVC\\nTechniques: {', '.join(technique) if technique else 'no techniques'}\\nKernel: {kernel}\\nC: {c}\\nClass weight: {class_weight}\\nAccuracy: {balanced_accuracy}\")\n",
    "    else:\n",
    "        fig.suptitle(f\"Correlation Matrix SVC\\nTechniques: {', '.join(technique) if technique else 'no techniques'}\\nKernel: {kernel}\\nC: {c}\\nGamma: {gamma}\\nClass weight: {class_weight}\\nAccuracy: {balanced_accuracy}\")\n",
    "    \n",
    "    # Si trasforma la matrice 3x3 in un array 1D\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Si itera per ogni coppia matrice - classe corrispondente\n",
    "    for i, (matrix, class_name) in enumerate(zip(mcm, class_names)):\n",
    "        \n",
    "        # Si salvano i valori True Negative (tn) False Positive (fp)\n",
    "        # False Negative (fn) e True Positive (tp)\n",
    "        tn, fp = matrix[0]\n",
    "        fn, tp = matrix[1]\n",
    "\n",
    "        # Si calcola l'accuratezza per ogni classe\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        \n",
    "        # Si mostra il seguente messaggio nella matrice di correlazione\n",
    "        display_labels = [f\"Non-{class_name}\", f\"{class_name}\"]\n",
    "\n",
    "        # Si visualizza la matrice di ogni classe con ConfusionMatrixDisplay\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=display_labels)\n",
    "        disp.plot(ax=axes[i], cmap=\"Reds\")\n",
    "        axes[i].set_title(f\"{class_name} vs Resto\\nAccuracy: {accuracy}\")\n",
    "\n",
    "    # Si visualizza la matrice di confusione generale normalizzata \n",
    "    # si vedono le percentuali di corretta previsione e dove il modello fa confusione\n",
    "    cm = confusion_matrix(y_test, y_pred, normalize=\"true\")\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(ax=axes[7], cmap=\"Reds\", values_format=\".2f\")\n",
    "    axes[7].set_title(f\"Confusion Matrix Normalized\")\n",
    "    axes[7].set_xticklabels(class_names, rotation=90)\n",
    "    \n",
    "    # Non si visualizza l'ultimo subplot disponibile\n",
    "    axes[8].set_visible(False)\n",
    "    \n",
    "    # Si salva l'immagine nell'apposita cartella\n",
    "    plt.figure(fig.number)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{dir}/{'_'.join(technique) if technique else 'no_techniques'}_confusion_matrix\")\n",
    "    plt.close(fig)\n",
    "    \n",
    "# Si converte results in un pandas DataFrame e si ordinano i risultati\n",
    "result_svc = pd.DataFrame(result_svc)\n",
    "print(result_svc.sort_values(\"accuracy\", ascending=False))\n",
    "\n",
    "# Si ricava la migliore accuracy e si stamapa a video\n",
    "print(\"\\nMiglior configurazione:\")\n",
    "best_idx = result_svc['accuracy'].idxmax()\n",
    "best_config_svc = result_svc.iloc[[best_idx]]\n",
    "print(best_config_svc[[\"technique\", \"accuracy\", \"kernel\", \"C\", \"gamma\", \"class_weight\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Miglior configurazione:\n",
    "    technique  accuracy kernel     C  gamma class_weight\n",
    "normalization  0.941211    rbf  10.0  scale         None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si salva la miglior configurazione trovata\n",
    "best_configs_list.append((\"SVC\", best_config_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Il Random Forest è un algoritmo di machine learning basato su una *collezione di alberi decisionali indipendenti*. Ogni albero viene costruito su un *sottoinsieme casuale dei dati* (bootstrap) e delle feature, creando una \"foresta\" di alberi che votano collettivamente per la predizione finale.\n",
    "\n",
    "I Random Forest offrono numerosi vantaggi: robustezza all'overfitting, capacità di gestire dataset complessi, alta accuratezza e capacità di stimare l'importanza delle feature. Possono elaborare simultaneamente feature numeriche e categoriche, tollerano valori mancanti e sono meno sensibili a outlier rispetto ai singoli alberi decisionali.\n",
    "\n",
    "La libreria *scikit-learn* fornisce la classe [*RandomForestClassifier*](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) per implementare l'algoritmo. Di default utilizza l'indice di Gini per misurare la qualità degli split e costruisce 100 alberi nella foresta.\n",
    "\n",
    "Si effettua un tuning degli iperparametri per trovare la migliore combinzaione di quest'ultimi (si utilizza un subset del dataset originale per ridurre la complessità).\n",
    "\n",
    "Si noti che *RandomForest* è un algoritmo costoso computazionalmente, di conseguenza di usa un subset del dataset originale (30%). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# skf = StratifiedKFold è già stato definito precedentemente\n",
    "\n",
    "# Si definisce una lista per contenere i risultati\n",
    "results_randomForest = []\n",
    "\n",
    "# Si definisce il classificatore RandomForestClassifier su cui effettuare\n",
    "# il tuning degli iperparametri. \n",
    "clf = RandomForestClassifier(random_state=21, n_jobs=-1)\n",
    "\n",
    "# Si defisce una lista del numero di alberi da testare\n",
    "n_estimators = [100, 200, 300]\n",
    "\n",
    "# Si definisce il criterio per misurare la qualità degli split\n",
    "criterion = ['gini', 'entropy']\n",
    "\n",
    "# Si definisce una lista di profondità di alberi da testare\n",
    "max_depth = [5, 10, 15]\n",
    "\n",
    "# Si definisce il parametro per la gestione dei pesi\n",
    "class_weight = ['balanced', 'balanced_subsample']\n",
    "\n",
    "# Si definisce un subset del dataset originale\n",
    "X_subset, _, y_subset, _ = train_test_split(X, y, train_size=0.3, random_state=21, stratify=y)\n",
    "\n",
    "# Si divide il subset del dataset originale in train e test con stratificazione \n",
    "# ovvero le occorrenze delle classi sono automaticamente bilanciate\n",
    "# tra train e test (random state garantisce la riproducibilità)\n",
    "X_train, _, y_train, _ = train_test_split(X_subset, y_subset, test_size=0.2, random_state=21, stratify=y_subset)\n",
    "\n",
    "# Si inizializzano le due variabili utilizzate successivamente per salvare il miglior\n",
    "# classificatore ttrovato tramite la grid search e cross validation\n",
    "best_randomForest_score = -1\n",
    "best_randomForest_clf = None\n",
    "\n",
    "# Si itera per tutte le combinazioni di tecniche di pre processing possibili \n",
    "# e precedentemente definite\n",
    "for technique in techniques:\n",
    "    \n",
    "    # Si definisce una lista per contenere i vari step per la cross validation\n",
    "    steps = []\n",
    "    \n",
    "    # Se la tecnica specificata tra virgolette è presente nella lista\n",
    "    # technique si aggiunge tra gli step della cross validation\n",
    "    if \"standardization\" in technique:\n",
    "        steps.append((\"scaler\", scaler))\n",
    "        \n",
    "        # Si salva il numero corretto di cluster per la standardizzazione \n",
    "        n_clusters = n_clusters_std\n",
    "        \n",
    "    elif \"normalization\" in technique:\n",
    "        steps.append((\"normalizer\", normalizer))\n",
    "        \n",
    "        # Si salva il numero corretto di cluster per la normalizzazione\n",
    "        n_clusters = n_clusters_std\n",
    "        \n",
    "    if \"agglomeration\" in technique:\n",
    "        \n",
    "        # Si definisce lo step dell'agglomerazione di feature con gli stessi \n",
    "        # parametri usati precedentemente \n",
    "        steps.append((\"agglomeration\", FeatureAgglomeration(\n",
    "            n_clusters=n_clusters,\n",
    "            metric=\"euclidean\",\n",
    "            linkage=\"ward\"\n",
    "        )))\n",
    "        \n",
    "    if \"undersampling\" in technique:\n",
    "        steps.append((\"undersampler\", cc))\n",
    "        \n",
    "    elif \"oversampling\" in technique:\n",
    "        steps.append((\"oversampler\", smote))\n",
    "    \n",
    "    # Si aggiunge il classificatore RandomForest come ultimo step\n",
    "    # della cross validation\n",
    "    steps.append((\"classifier\", clf))\n",
    "    \n",
    "    # Si crea la pipeline di step \n",
    "    # si utilizza ImbPipeline invece di Pipeline in quanto accetta \n",
    "    # le tecniche di sampling (oversampling/undersampling)\n",
    "    pipeline = ImbPipeline(steps)\n",
    "    \n",
    "    # Si definisce il parametro di GridSearchCV per la ricerca degli\n",
    "    # iperparametri migliori\n",
    "    param_grid = {\n",
    "        \"classifier__n_estimators\": n_estimators,\n",
    "        \"classifier__criterion\": criterion,\n",
    "        \"classifier__max_depth\": max_depth,\n",
    "        \"classifier__class_weight\": class_weight\n",
    "    }\n",
    "    \n",
    "    # Si definisce GridSearchCV, la pipeline da seguire, i parametri,\n",
    "    # la tecnica per la cross validation (StratifiedKFold), lo scoring da \n",
    "    # utilizzare (balanced_accuracy in quanto il dataset è sbilanciato) e il\n",
    "    # numero di core da utilizzare contemporaneamente\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=skf,\n",
    "        scoring=\"balanced_accuracy\",\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Si esegue la cross validation sul train set, per ogni tecnica si \n",
    "    # verifica quale combinazione di iperparametri ottiene l'accuratezza maggiore\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Si verifica il risultato del modello, se migliore del precedente si salva il corrente\n",
    "    if grid_search.best_score_ > best_randomForest_score:\n",
    "        best_randomForest_score = grid_search.best_score_\n",
    "        best_randomForest_clf = grid_search.best_estimator_\n",
    "        \n",
    "    # Si estrae solo il miglior risultato per questa tecnica\n",
    "    best_params = grid_search.best_params_\n",
    "        \n",
    "    # Si definiscono i risultati per ogni tecnica, profondità e criterio\n",
    "    result = {\n",
    "        \"techniques_list\": technique,                           # Si mantiene la lista di tecniche utilizzate per comodità\n",
    "        \"techniques\": ', '.join(technique),                     # Si converte la lista di tecniche in stringa per migliore visualizzazione\n",
    "        \"n_estimators\": best_params[\"classifier__n_estimators\"],     # Si salva il valore degli alberi utilizzati\n",
    "        \"criterion\": best_params[\"classifier__criterion\"],           # Si salva il parametro criterion\n",
    "        \"max_depth\": best_params[\"classifier__max_depth\"],           # Si salva la profondità\n",
    "        \"class_weight\": best_params[\"classifier__class_weight\"],     # Si salva il peso utilizzato\n",
    "        \"mean_test_score\": grid_search.best_score_,                     # Si salva il valore medio dell'accuratezza bilanciata ottenuto dalle 5 iterazioni della cross validation\n",
    "        \"std_test_score\": grid_search.cv_results_[\"std_test_score\"][    # Si salva la deviazione standard del miglior punteggio\n",
    "            grid_search.cv_results_[\"mean_test_score\"].argmax()\n",
    "        ]\n",
    "    }\n",
    "        \n",
    "    # Si aggiungono ai risultati finali\n",
    "    results_randomForest.append(result)\n",
    "\n",
    "    # Si ibera la memoria\n",
    "    del grid_search\n",
    "        \n",
    "# Si trasforma result in un pandas DataFrame\n",
    "results_randomForest = pd.DataFrame(results_randomForest)\n",
    "\n",
    "# Si restituisce la combinazione dei risultati migliori per ogni tecnica\n",
    "print(\"\\nMigliori configurazioni per ogni tecnica:\")\n",
    "print(results_randomForest[[\"techniques\", \"n_estimators\", \"criterion\", \"max_depth\", \"class_weight\", \"mean_test_score\", \"std_test_score\"]].sort_values(\"mean_test_score\", ascending=False)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Migliori configurazioni per ogni tecnica:\n",
    "                                   techniques  n_estimators criterion  max_depth        class_weight  mean_test_score  std_test_score\n",
    "                                 oversampling           200      gini         15            balanced         0.938481        0.004535\n",
    "                  normalization, oversampling           200   entropy         15  balanced_subsample         0.936801        0.002530\n",
    "                standardization, oversampling           100   entropy         10            balanced         0.935922        0.004005\n",
    "                                                        100      gini         10            balanced         0.934762        0.007746\n",
    "                                normalization           300      gini         15            balanced         0.934678        0.005034\n",
    "                              standardization           200   entropy         10  balanced_subsample         0.934559        0.006729\n",
    "               standardization, undersampling           100      gini         10  balanced_subsample         0.929389        0.003937\n",
    "                 normalization, undersampling           100   entropy          5  balanced_subsample         0.927122        0.003353\n",
    "               standardization, agglomeration           100   entropy         15            balanced         0.918924        0.003384\n",
    "standardization, agglomeration, undersampling           100   entropy         15  balanced_subsample         0.918894        0.005151\n",
    " standardization, agglomeration, oversampling           300   entropy         15            balanced         0.917825        0.003587\n",
    "                 normalization, agglomeration           100   entropy         10            balanced         0.911775        0.005776\n",
    "   normalization, agglomeration, oversampling           300   entropy         15            balanced         0.907145        0.010949\n",
    "  normalization, agglomeration, undersampling           300   entropy         10            balanced         0.905533        0.007756\n",
    "                                undersampling           300   entropy         10            balanced         0.902550        0.003993\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_randomForest_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si verificano i risultati del codice precedente:\n",
    "- Per ogni configurazione trovata, si applicano le medesime tecniche di preprocessing e si addestra un algoritmo RandomForest con gli ipeparametri ottimali.\n",
    "- Si esegue una valutazione delle performance sul test set.\n",
    "- Si visualizzano le matrici di confusione per ogni configurazione.\n",
    "- Si salvano i risultati della migliore configurazione. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si crea la directory per RANDOM FOREST se non esiste\n",
    "dir = \"9 - RANDOM FOREST\"\n",
    "\n",
    "path_dir = os.path.join(os.getcwd(), dir)\n",
    "\n",
    "if not os.path.exists(path_dir):\n",
    "    os.makedirs(path_dir)\n",
    "\n",
    "# Si crea una lista per contenere i vari risultati\n",
    "result_randomForest = []\n",
    "\n",
    "# Si ridefinisce il medesimo subset per lavorare con il test set corretto\n",
    "X_subset, _, y_subset, _ = train_test_split(X, y, train_size=0.3, random_state=21, stratify=y)\n",
    "\n",
    "# Si itera lungo le righe delle configurazioni migliori \n",
    "for row in results_randomForest.itertuples():\n",
    "    # Si salva la lista contenente le tecniche di pre processing \n",
    "    technique = row.techniques_list\n",
    "    \n",
    "    # Si divide il subset del dataset in train e test con stratificazione\n",
    "    # random_state = 21 ci garantisce che per ogni iterazione otteniamo \n",
    "    # sempre le stesse separazioni e si combinano le varie tecniche di preprocessing\n",
    "    X_train, X_test, y_train, y_test = combine_preprocessing(X_subset, y_subset, technique)\n",
    "    \n",
    "    # Si salva il valore di n_estimators\n",
    "    n_estimators = row.n_estimators\n",
    "    \n",
    "    # Si salva il parametro criterion\n",
    "    criterion = row.criterion\n",
    "    \n",
    "    # Si salva la profondità\n",
    "    max_depth = row.max_depth\n",
    "    \n",
    "    # Si salva il parametro utilizzato per i pesi\n",
    "    class_weight = row.class_weight\n",
    "    \n",
    "    # Si definisce RandomForest con gli iperparametri definiti\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, class_weight=class_weight, random_state=21, n_jobs=-1)\n",
    "    \n",
    "    # Si addestra il modello RandomForest sul train set\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Si salvano le predizioni effettuate\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Si salva l'accuratezza bilanciata del modello\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Si salvano i vari risultati d'interesse\n",
    "    result = {\n",
    "        \"techniques_list\": technique,\n",
    "        \"technique\": ', '.join(technique) if technique else 'no techniques',\n",
    "        \"accuracy\": balanced_accuracy,\n",
    "        \"n_estimators\": n_estimators,\n",
    "        \"criterion\": criterion,\n",
    "        \"max_depth\": max_depth,\n",
    "        \"class_weight\": class_weight\n",
    "    }\n",
    "    \n",
    "    # Si aggiungono ai risultati finali\n",
    "    result_randomForest.append(result)\n",
    "    \n",
    "    # Si definiscono le matrici binarie (1 vs All)\n",
    "    mcm = multilabel_confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Si salvano i nomi delle classi\n",
    "    class_names = clf.classes_\n",
    "    \n",
    "    # Si crea una figura con 3 righe e 3 colonne di subplot\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    \n",
    "    # Si inserisce il titolo personalizzato\n",
    "    fig.suptitle(f\"Correlation Matrix RandomForest\\nTechniques: {', '.join(technique) if technique else 'no techniques'}\\nTrees: {n_estimators}\\nCriterion: {criterion}\\Profondità: {max_depth}\\nClass weight: {class_weight}\\nAccuracy: {balanced_accuracy}\")\n",
    "    \n",
    "    # Si trasforma la matrice 3x3 in un array 1D\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Si itera per ogni coppia matrice - classe corrispondente\n",
    "    for i, (matrix, class_name) in enumerate(zip(mcm, class_names)):\n",
    "        \n",
    "        # Si salvano i valori True Negative (tn) False Positive (fp)\n",
    "        # False Negative (fn) e True Positive (tp)\n",
    "        tn, fp = matrix[0]\n",
    "        fn, tp = matrix[1]\n",
    "\n",
    "        # Si calcola l'accuratezza per ogni classe\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        \n",
    "        # Si mostra il seguente messaggio nella matrice di correlazione\n",
    "        display_labels = [f\"Non-{class_name}\", f\"{class_name}\"]\n",
    "\n",
    "        # Si visualizza la matrice di ogni classe con ConfusionMatrixDisplay\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=display_labels)\n",
    "        disp.plot(ax=axes[i], cmap=\"Reds\")\n",
    "        axes[i].set_title(f\"{class_name} vs Resto\\nAccuracy: {accuracy}\")\n",
    "\n",
    "    # Si visualizza la matrice di confusione generale normalizzata \n",
    "    # si vedono le percentuali di corretta previsione e dove il modello fa confusione\n",
    "    cm = confusion_matrix(y_test, y_pred, normalize=\"true\")\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(ax=axes[7], cmap=\"Reds\", values_format=\".2f\")\n",
    "    axes[7].set_title(f\"Confusion Matrix Normalized\")\n",
    "    axes[7].set_xticklabels(class_names, rotation=90)\n",
    "    \n",
    "    # Non si visualizza l'ultimo subplot disponibile\n",
    "    axes[8].set_visible(False)\n",
    "    \n",
    "    # Si salva l'immagine nell'apposita cartella\n",
    "    plt.figure(fig.number)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{dir}/{'_'.join(technique) if technique else 'no_techniques'}_confusion_matrix\")\n",
    "    plt.close(fig)\n",
    "    \n",
    "# Si converte results in un pandas DataFrame e si ordinano i risultati\n",
    "result_randomForest = pd.DataFrame(result_randomForest)\n",
    "print(result_randomForest.sort_values(\"accuracy\", ascending=False))\n",
    "\n",
    "# Si ricava la migliore accuracy e si stamapa a video\n",
    "print(\"\\nMiglior configurazione:\")\n",
    "best_idx = result_randomForest['accuracy'].idxmax()\n",
    "best_config_randomForest = result_randomForest.iloc[[best_idx]]\n",
    "print(best_config_randomForest[[\"technique\", \"accuracy\", \"n_estimators\", \"criterion\", \"max_depth\", \"class_weight\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Miglior configurazione:\n",
    "                    technique  accuracy  n_estimators criterion  max_depth class_weight\n",
    "standardization, oversampling  0.938518           100   entropy         10     balanced\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si salva la miglior configurazione trovata\n",
    "best_configs_list.append((\"RandomForest\", best_config_randomForest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier\n",
    "Il Voting Classifier è un classificatore appartenente alla categorio degli *ensemble learning methods* ovvero quei classificatori che combinano vari modelli di apprendimento per migliorare le prestazioni (come il *Random Forest*).\n",
    "\n",
    "Il Voting classifier funziona tramite due modalità principali: \n",
    "- *Hard Voting*: in cui ogni classificatore all'interno del Voting classifier esprime un voto per una classe, la classe che ottiene il maggior numero di voti è la classe finale.\n",
    "- *Soft Voting*: in cui ogni classificatore all'interno del Voting Classifier deve fornire la probabilità di appartenenza ad ogni classe, la classe con la probabilità media più alta viene scelta (metodo valido solo con i classificatori che restituiscono le probabilità per classe).\n",
    "\n",
    "la libreria *scikit-learn* mette a disposizione la classe [*VotingClassifier*](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html) per implementare l'algoritmo. By default presenta il sistema di voto impostato su *hard* e i pesi impostati su *None* (ovvero tutte le classi hanno lo stesso peso).\n",
    "\n",
    "Si definiscono due differenti *GridSearchCV* per il tuning degli iperparametri, uno dedicato al *soft voting* e uno all'*hard voting*, questo perchè il modello *SVC* non fornisce le probabilità per classe by default e il calcolo è estremamente costoso computazionalmente. Si effettua il tuning degli iperparametri sui pesi del modello, testando varie soluzioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# skf = StratifiedKFold è già stato definito precedentemente\n",
    "\n",
    "# Si salvano gli score ottenuti dai vari modelli per usarli come pesi\n",
    "best_score_hard = [best_decisionTree_score, best_knn_score, best_naiveBayes_score, best_svc_score, best_randomForest_score]\n",
    "best_score_soft = [best_decisionTree_score, best_knn_score, best_naiveBayes_score, best_randomForest_score]\n",
    "\n",
    "# Si definisce una lista per contenere i risultati (hard e soft voting)\n",
    "results_votingClassifier = []\n",
    "\n",
    "# Si creano due VotingClassifier separati\n",
    "# Si crea il classificatore per hard voting (con tutti i classificatori)\n",
    "hard_voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"decisiontree\", best_decisionTree_clf),\n",
    "        (\"knn\", best_knn_clf),\n",
    "        (\"naivebayes\", best_naiveBayes_clf),\n",
    "        (\"svc\", best_svc_clf),\n",
    "        (\"randomforest\", best_randomForest_clf)\n",
    "    ],\n",
    "    voting='hard',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Si crea il classificatore per soft voting (senza SVC, in quanto non ha probability = True)\n",
    "soft_voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"decisiontree\", best_decisionTree_clf),\n",
    "        (\"knn\", best_knn_clf),\n",
    "        (\"naivebayes\", best_naiveBayes_clf),\n",
    "        (\"randomforest\", best_randomForest_clf)\n",
    "    ],\n",
    "    voting='soft',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Si divide il dataset iniziale in train e test con stratificazione \n",
    "# ovvero le occorrenze delle classi sono automaticamente bilanciate\n",
    "# tra train e test (random state garantisce la riproducibilità)\n",
    "X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)\n",
    "\n",
    "# Si impostano i parametri dei pesi per l'hard voting\n",
    "hard_param_grid = {\n",
    "    \"weights\": [\n",
    "        None,\n",
    "        best_score_hard,\n",
    "        [1, 1, 1, 1, 0],\n",
    "        [1, 1, 1, 0, 1],\n",
    "        [1, 1, 0, 1, 1],\n",
    "        [1, 0, 1, 1, 1],\n",
    "        [0, 1, 1, 1, 1],\n",
    "        # Si impostano i pesi in base ai risultati ottenuti dai vari\n",
    "        # classificatori, 5 per quello con l'accuratezza maggiore, 1 per\n",
    "        # quello con l'accuratezza minore.\n",
    "        [2, 3, 1, 5, 4]\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Si impostano i parametri dei pesi per soft voting\n",
    "soft_param_grid = {\n",
    "    \"weights\": [\n",
    "        None,\n",
    "        best_score_soft,\n",
    "        [1, 1, 1, 0],\n",
    "        [1, 1, 0, 1],\n",
    "        [1, 0, 1, 1],\n",
    "        [0, 1, 1, 1],\n",
    "        # Si impostano i pesi in base ai risultati ottenuti dai vari\n",
    "        # classificatori, 4 per quello con l'accuratezza maggiore, 1 per\n",
    "        # quello con l'accuratezza minore.\n",
    "        [2, 3, 1, 4]\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Si esegue la GridSearchCV con hard voting\n",
    "hard_grid_search = GridSearchCV(\n",
    "    estimator=hard_voting_clf,\n",
    "    param_grid=hard_param_grid,\n",
    "    cv=skf,\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Si esegue la GridSearchCV con soft voting\n",
    "soft_grid_search = GridSearchCV(\n",
    "    estimator=soft_voting_clf,\n",
    "    param_grid=soft_param_grid,\n",
    "    cv=skf,\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Si esegue la cross validation sul train set per entrambi i classificatori\n",
    "hard_grid_search.fit(X_train, y_train)\n",
    "soft_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Si estraggono i modelli migliori\n",
    "best_hardVoting_clf = hard_grid_search.best_estimator_\n",
    "best_softVoting_clf = soft_grid_search.best_estimator_\n",
    "\n",
    "# Si estraggono dei risultati per hard voting\n",
    "for params, mean_test_score, std_test_score in zip(\n",
    "    hard_grid_search.cv_results_[\"params\"],\n",
    "    hard_grid_search.cv_results_[\"mean_test_score\"],\n",
    "    hard_grid_search.cv_results_[\"std_test_score\"]\n",
    "):\n",
    "    # Si definiscono i risultati per ogni combinazione\n",
    "    result = {\n",
    "        \"voting\": \"hard\",                  # Si mantiene la tipologia di voting\n",
    "        \"weights\": params[\"weights\"],           # Si salva la lista dei pesi utilizzata\n",
    "        \"mean_test_score\": mean_test_score,     # Si salva il valore dell'accuratezza bilanciata media\n",
    "        \"std_test_score\": std_test_score        # Si salva il valore della deviazione standard\n",
    "    }\n",
    "    \n",
    "    # Si aggiungono i risultati\n",
    "    results_votingClassifier.append(result)\n",
    "\n",
    "# Si libera la memoria\n",
    "del hard_grid_search\n",
    "\n",
    "# Si estraggono i risultati per soft voting\n",
    "for params, mean_test_score, std_test_score in zip(\n",
    "    soft_grid_search.cv_results_[\"params\"],\n",
    "    soft_grid_search.cv_results_[\"mean_test_score\"],\n",
    "    soft_grid_search.cv_results_[\"std_test_score\"]\n",
    "):\n",
    "    # Si definiscono i risultati per ogni combinazione\n",
    "    result = {\n",
    "        \"voting\": \"soft\",                  # Si mantiene la tipologia di voting\n",
    "        \"weights\": params[\"weights\"],           # Si salva la lista dei pesi utilizzata\n",
    "        \"mean_test_score\": mean_test_score,     # Si salva il valore dell'accuratezza bilanciata medi\n",
    "        \"std_test_score\": std_test_score        # Si salva il valore della deviazione standard\n",
    "    }\n",
    "    \n",
    "    # Si aggiungono i risultati\n",
    "    results_votingClassifier.append(result)\n",
    "\n",
    "# Si libera la memoria\n",
    "del soft_grid_search\n",
    "\n",
    "# Si trasformano i risultati un pandas DataFrame\n",
    "results_votingClassifier = pd.DataFrame(results_votingClassifier)\n",
    "\n",
    "# Si stampano i risultati in ordine di accuratezza\n",
    "print(\"\\nMigliori configurazioni:\")\n",
    "print(results_votingClassifier.sort_values(\"mean_test_score\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Migliori configurazioni:\n",
    "voting            weights  mean_test_score  std_test_score\n",
    "  hard    [2, 3, 1, 5, 4]         0.937521        0.003991\n",
    "  hard  [best_score_hard]         0.936997        0.004204\n",
    "  hard               None         0.936980        0.004322\n",
    "  hard    [1, 1, 0, 1, 1]         0.936798        0.004236\n",
    "  hard    [0, 1, 1, 1, 1]         0.936356        0.002167\n",
    "  soft       [2, 3, 1, 4]         0.936307        0.003814\n",
    "  soft       [1, 1, 0, 1]         0.935234        0.005741\n",
    "  hard    [1, 1, 1, 0, 1]         0.934846        0.003369\n",
    "  hard    [1, 1, 1, 1, 0]         0.934699        0.002747\n",
    "  hard    [1, 0, 1, 1, 1]         0.933753        0.004116\n",
    "  soft  [best_score_soft]         0.932765        0.002429\n",
    "  soft               None         0.932373        0.002199\n",
    "  soft       [0, 1, 1, 1]         0.930848        0.002818\n",
    "  soft       [1, 1, 1, 0]         0.929316        0.002360\n",
    "  soft       [1, 0, 1, 1]         0.928375        0.002083\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hardVoting_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_softVoting_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea directory per VOTING CLASSIFIER se non esiste \n",
    "dir = \"10 - VOTING CLASSIFIER\"\n",
    "\n",
    "path_dir = os.path.join(os.getcwd(), dir)\n",
    "\n",
    "if not os.path.exists(path_dir):\n",
    "    os.makedirs(path_dir)\n",
    "    \n",
    "# Si crea una lista per conservare i risultati\n",
    "result_votingClassifier = []\n",
    "\n",
    "# Si itera lungo le righe delle configurazioni migliori \n",
    "for i, row in enumerate(results_votingClassifier.itertuples()):\n",
    "    \n",
    "    # Si salva la tipologia di voting\n",
    "    voting = row.voting\n",
    "    \n",
    "    # Si salvano i pesi utilizzati\n",
    "    weights = row.weights\n",
    "    \n",
    "    # Si divide il dataset iniziale in train e test con stratificazione \n",
    "    # ovvero le occorrenze delle classi sono automaticamente bilanciate\n",
    "    # tra train e test (random state garantisce la riproducibilità)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)\n",
    "\n",
    "    if voting == \"soft\":\n",
    "        clf = VotingClassifier(\n",
    "            estimators=[\n",
    "                (\"decisiontree\", best_decisionTree_clf),\n",
    "                (\"knn\", best_knn_clf),\n",
    "                (\"naivebayes\", best_naiveBayes_clf),\n",
    "                (\"randomforest\", best_randomForest_clf)\n",
    "            ],\n",
    "            voting=voting,\n",
    "            weights=weights,\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "    elif voting == \"hard\":\n",
    "        clf = VotingClassifier(\n",
    "            estimators=[\n",
    "                (\"decisiontree\", best_decisionTree_clf),\n",
    "                (\"knn\", best_knn_clf),\n",
    "                (\"naivebayes\", best_naiveBayes_clf),\n",
    "                (\"svc\", best_svc_clf),\n",
    "                (\"randomforest\", best_randomForest_clf)\n",
    "            ],\n",
    "            voting=voting,\n",
    "            weights=weights,\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )    \n",
    "    \n",
    "    # Si addestra il modello  \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Si salvano le predizioni effettuate\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Si salva l'accuratezza bilanciata del modello\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Si salvano i valori di interesse\n",
    "    result = {\n",
    "        \"accuracy\": balanced_accuracy,\n",
    "        \"voting\": voting,\n",
    "        \"weights\": weights\n",
    "    }\n",
    "    \n",
    "    # Si aggiungono ai risultati\n",
    "    result_votingClassifier.append(result)\n",
    "    \n",
    "    # Si definiscono le matrici binarie (1 vs All)\n",
    "    mcm = multilabel_confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Si salvano i nomi delle classi\n",
    "    class_names = clf.classes_\n",
    "    \n",
    "    # Si crea una figura con 3 righe e 3 colonne di subplot\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    \n",
    "    # Si inserisce il titolo personalizzato\n",
    "    fig.suptitle(f\"Correlation Matrix VotingClassifier\\nVoting: {voting}\\nWeights: {weights}\")\n",
    "    \n",
    "    # Si trasforma la matrice 3x3 in un array 1D\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Si itera per ogni coppia matrice - classe corrispondente\n",
    "    for j, (matrix, class_name) in enumerate(zip(mcm, class_names)):\n",
    "        \n",
    "        # Si salvano i valori True Negative (tn) False Positive (fp)\n",
    "        # False Negative (fn) e True Positive (tp)\n",
    "        tn, fp = matrix[0]\n",
    "        fn, tp = matrix[1]\n",
    "\n",
    "        # Si calcola l'accuratezza per ogni classe\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        \n",
    "        # Si mostra il seguente messaggio nella matrice di correlazione\n",
    "        display_labels = [f\"Non-{class_name}\", f\"{class_name}\"]\n",
    "\n",
    "        # Si visualizza la matrice di ogni classe con ConfusionMatrixDisplay\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=display_labels)\n",
    "        disp.plot(ax=axes[j], cmap=\"Reds\")\n",
    "        axes[j].set_title(f\"{class_name} vs Resto\\nAccuracy: {accuracy}\")\n",
    "\n",
    "    # Si visualizza la matrice di confusione generale normalizzata \n",
    "    # si vedono le percentuali di corretta previsione e dove il modello fa confusione\n",
    "    cm = confusion_matrix(y_test, y_pred, normalize=\"true\")\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(ax=axes[7], cmap=\"Reds\", values_format=\".2f\")\n",
    "    axes[7].set_title(f\"Confusion Matrix Normalized\")\n",
    "    axes[7].set_xticklabels(class_names, rotation=90)\n",
    "    \n",
    "    # Non si visualizza l'ultimo subplot disponibile\n",
    "    axes[8].set_visible(False)\n",
    "    \n",
    "    # Si salva l'immagine nell'apposita cartella\n",
    "    plt.figure(fig.number)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{dir}/{i}_confusion_matrix\")\n",
    "    plt.close(fig)\n",
    "\n",
    "# Si converte results in un pandas DataFrame e si ordinano i risultati\n",
    "result_votingClassifier = pd.DataFrame(result_votingClassifier)\n",
    "print(result_votingClassifier.sort_values(\"accuracy\", ascending=False))\n",
    "\n",
    "# Si ricava la migliore accuracy e si stamapa a video\n",
    "print(\"\\nMiglior configurazione:\")\n",
    "best_idx = result_votingClassifier['accuracy'].idxmax()\n",
    "best_config_votingClassifier = result_votingClassifier.iloc[[best_idx]]\n",
    "print(best_config_votingClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Miglior configurazione:\n",
    "accuracy voting          weights\n",
    "0.941895   hard  [1, 1, 0, 1, 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si salva la miglior configurazione trovata\n",
    "best_configs_list.append((\"VotingClassifier\", best_config_votingClassifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prova = [1, 2, 3, 4, 5, 6]\n",
    "print(len(prova))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusioni\n",
    "\n",
    "Per arrivare alla **conclusione** sono stati effettuati vari passaggi. Primo fra tutti il **tuning degli iperparametri** per gli algoritmi di classificazione *DecisionTree, K-Nearest Neighbors, Naive Bayes (Gaussian), Support Vector Classifier, Random Forest e Voting Classifier* sulle varie combinazioni di tecniche di **pre-processing**:\n",
    "```txt\n",
    "- Nessuna,\n",
    "- standardization,\n",
    "- normalization,\n",
    "- undersampling,\n",
    "- oversampling,\n",
    "- standardization + undersampling,\n",
    "- standardization + oversampling,\n",
    "- normalization + undersampling,\n",
    "- normalization + oversampling,\n",
    "- standardization + agglomeration,\n",
    "- normalization + agglomeration,\n",
    "- standardization + agglomeration + undersampling,\n",
    "- standardization + agglomeration + oversampling,\n",
    "- normalization + agglomeration + undersampling,\n",
    "- normalization + agglomeration + oversampling.\n",
    "```\n",
    "e su varie combinazioni di **iperparametri**:\n",
    "```txt\n",
    "- Decision Tree:\n",
    "    -> max_depth: profondità dell'albero di classificazione;\n",
    "    -> criterion: misura di qualità degli split.\n",
    "- K-Nearest Neighbors: \n",
    "    -> n_neighbors: numero di istanze più prossime per la classificazione;\n",
    "    -> weights: modalità di ponderazione delle istanze più prossime;\n",
    "    -> metric: funzione di distanza utilizzata per misurare la prossimità tra punti nello spazio.\n",
    "- Naive Bayes (Gaussian):\n",
    "    -> var_smoothing: varianza massima delle caratteristiche aggiunta alle varianze per garantire stabilità.\n",
    "- Support Vector Classifier:\n",
    "    -> kernel: funzione per mappare i dati separandoli linearmente;\n",
    "    -> C: penalità dell'errore di classificazione;\n",
    "    -> gamma: influenza di ciascun punto di training;\n",
    "    -> class_weight: bilancia l'importanza delle classi.\n",
    "- Random Forest:\n",
    "    -> n_estimator: numero di alberi decisionali;\n",
    "    -> criterion: misura di qualità degli split;\n",
    "    -> max_depth: profondità massima degli alberi;\n",
    "    -> class_weight: bilancia l'importanza delle classi.\n",
    "- Voting Classifier:\n",
    "    -> voting: strategia di aggregazione delle predizioni;\n",
    "    -> weights: bilancia l'importanza delle classi.\n",
    "```\n",
    "Il **tuning degli iperparametri** è stato eseguito sul medesimo **Train set** tramite cross validation (*tecnica di valutazione che suddivide iterativamente il dataset in sottoinsiemi complementari per l'addestramento e la validazione*).\n",
    "\n",
    "Successivamente è stata effettuata la valutazione sul medesimo **Test set**, salvando i migliori risultati ottenuti per ogni algoritmo e configurazione.\n",
    "\n",
    "L'intero processo ha lo scopo di determinare quale sia la migliore ottimizzazione algoritmica (come combinazione di tecniche di pre-elaborazione e configurazioni parametriche) per l'implementazione del più accurato modello di Machine Learning per il problema di classificazione multiclasse del dataset **dry_bean**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si riordinano i modelli in ordine di accuracy\n",
    "best_configs_list = sorted(best_configs_list, key=lambda x: x[1]['accuracy'].item(), reverse=True)\n",
    "\n",
    "# Si itera lungo best model per una stampa personalizzata\n",
    "for i, (model_name, model_info) in enumerate(best_configs_list):\n",
    "    \n",
    "    # Si estrapola l'accuracy\n",
    "    accuracy = model_info['accuracy'].item()\n",
    "    \n",
    "    # Se stiamo lavorando sul modello migliore\n",
    "    if i == 0:\n",
    "        print(f\"Il miglior modello ottenuto è {model_name} con accuracy: {accuracy:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\nSeguito da {model_name} con accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Si stampa la tecnica utilizzata (se presente)\n",
    "    if 'technique' in model_info.columns:\n",
    "        technique = model_info['technique'].item()\n",
    "        print(f\"Tecniche di pre-processing utilizzate: {technique}\")\n",
    "    \n",
    "    print(\"Iperparametri:\")\n",
    "    # Si stampano informazioni specifiche per ciascun tipo di modello\n",
    "    if model_name == 'DecisionTree':\n",
    "        print(f\"   {'Max Depth:':<15} {model_info['max_depth'].item()}\")\n",
    "        print(f\"   {'Criterion:':<15} {model_info['criterion'].item()}\")\n",
    "    \n",
    "    elif model_name == 'KNN':\n",
    "        print(f\"   {'Neighbors:':<15} {model_info['k_neighbors'].item()}\")\n",
    "        print(f\"   {'Weights:':<15} {model_info['weights'].item()}\")\n",
    "        print(f\"   {'Metric:':<15} {model_info['metric'].item()}\")\n",
    "    \n",
    "    elif model_name == 'NaiveBayes':\n",
    "        print(f\"   {'Var Smoothing:':<15} {model_info['var_smoothing'].item()}\")\n",
    "        print(f\"   {'Priors:':<15} {'Weights' if model_info['priors'].item() is not None else 'None'}\")\n",
    "    \n",
    "    elif model_name == 'SVC':\n",
    "        print(f\"   {'Kernel:':<15} {model_info['kernel'].item()}\")\n",
    "        print(f\"   {'C:':<15} {model_info['C'].item()}\")\n",
    "        print(f\"   {'Gamma:':<15} {model_info['gamma'].item()}\")\n",
    "        print(f\"   {'Class Weight:':<15} {model_info['class_weight'].item()}\")\n",
    "    \n",
    "    elif model_name == 'RandomForest':\n",
    "        print(f\"   {'N Estimators:':<15} {model_info['n_estimators'].item()}\")\n",
    "        print(f\"   {'Criterion:':<15} {model_info['criterion'].item()}\")\n",
    "        print(f\"   {'Max Depth:':<15} {model_info['max_depth'].item()}\")\n",
    "        print(f\"   {'Class Weight:':<15} {model_info['class_weight'].item()}\")\n",
    "    \n",
    "    elif model_name == 'VotingClassifier':\n",
    "        print(f\"   {'Voting Type:':<15} {model_info['voting'].item()}\")\n",
    "        print(f\"   {'Weights:':<15} {model_info['weights'].item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Il miglior modello ottenuto è VotingClassifier con accuracy: 0.9419\n",
    "Iperparametri:\n",
    "   Voting Type:    hard\n",
    "   Weights:        [1, 1, 0, 1, 1]\n",
    "\n",
    "Seguito da SVC con accuracy: 0.9412\n",
    "Tecniche di pre-processing utilizzate: normalization\n",
    "Iperparametri:\n",
    "   Kernel:         rbf\n",
    "   C:              10.0\n",
    "   Gamma:          scale\n",
    "   Class Weight:   None\n",
    "\n",
    "Seguito da KNN con accuracy: 0.9365\n",
    "Tecniche di pre-processing utilizzate: standardization, oversampling\n",
    "Iperparametri:\n",
    "   Neighbors:      14\n",
    "   Weights:        distance\n",
    "   Metric:         euclidean\n",
    "\n",
    "Seguito da DecisionTree con accuracy: 0.9260\n",
    "Tecniche di pre-processing utilizzate: standardization, oversampling\n",
    "Iperparametri:\n",
    "   Max Depth:      9\n",
    "   Criterion:      gini\n",
    "\n",
    "Seguito da NaiveBayes con accuracy: 0.9100\n",
    "Tecniche di pre-processing utilizzate: oversampling\n",
    "Iperparametri:\n",
    "   Var Smoothing:  1e-16\n",
    "   Priors:         Weights\n",
    "\n",
    "Seguito da RandomForest con accuracy: 0.8816\n",
    "Tecniche di pre-processing utilizzate: normalization, agglomeration, oversampling\n",
    "Iperparametri:\n",
    "   N Estimators:   300\n",
    "   Criterion:      entropy\n",
    "   Max Depth:      15\n",
    "   Class Weight:   balanced\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
